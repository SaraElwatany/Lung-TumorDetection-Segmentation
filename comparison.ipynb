{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8MSR1S0uZhrFwj6vMgheP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraElwatany/Lung-TumorDetection-Segmentation/blob/main/comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import math\n",
        "import yaml\n",
        "import glob\n",
        "import random\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "fm2NBx1zVj4v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjhIHHVlXyCR",
        "outputId": "08348046-fb9a-4467-c4eb-1a5cf5ce6266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i_zlPVxYX5Wf"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdWWrZNadOiV"
      },
      "source": [
        "#### **DataLoading & Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E4-J0tPoYTEZ"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/LungTumorDetectionAndSegmentation'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation on Whole Images**"
      ],
      "metadata": {
        "id": "ayGrAmJ7gI3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5VJ0sUlQY9hh"
      },
      "outputs": [],
      "source": [
        "class LungTumorSegmentationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_path, transform=None, mask_transform=None):\n",
        "\n",
        "        self.images = []\n",
        "        for subject in os.listdir(os.path.join(root_path, 'images')):\n",
        "            subject_path = os.path.join(root_path, 'images', subject)\n",
        "            for image_file in os.listdir(subject_path):\n",
        "                self.images.append(os.path.join(subject_path, image_file))\n",
        "\n",
        "        self.masks = [img_path.replace('images', 'masks') for img_path in self.images]\n",
        "        self.transform = transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path = self.images[idx]\n",
        "        mask_path = self.masks[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"L\")  # grayscale image\n",
        "        mask = Image.open(mask_path).convert(\"L\")    # mask as single-channel\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "            mask = (mask > 0.5).float()  # Ensure binary values\n",
        "\n",
        "        return image_path, image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IgDRAQseciyk"
      },
      "outputs": [],
      "source": [
        "image_transform = T.Compose([\n",
        "                              T.Resize((256, 256)),\n",
        "                              T.ToTensor(),\n",
        "                           ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PAfvUkWEcEoE"
      },
      "outputs": [],
      "source": [
        "train_dataset = LungTumorSegmentationDataset(os.path.join(dataset_path, 'train'), image_transform, image_transform)\n",
        "val_dataset = LungTumorSegmentationDataset(os.path.join(dataset_path, 'val'), image_transform, image_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHQLI57Hc0ob",
        "outputId": "1b009e21-70bf-40b2-9691-a8c678d417f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the training data: 1832\n",
            "Length of the validation data: 98\n"
          ]
        }
      ],
      "source": [
        "print('Length of the training data:', len(train_dataset))\n",
        "print('Length of the validation data:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGvSiUXwfOc4",
        "outputId": "89830a9f-44b3-4a97-86fa-2272121432b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the training data: 1832\n",
            "Length of the validation data: 98\n"
          ]
        }
      ],
      "source": [
        "print('Length of the training data:', len(train_dataset))\n",
        "print('Length of the validation data:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qXdijYusfbTM"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYKKvIwIf7Bq",
        "outputId": "e73610c2-8a07-48ea-b5da-361298e2ca0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first training sample & corresponding mask: torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n",
            "Shape of the first validation sample & corresponding mask: torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "train_batches = iter(train_dataloader)\n",
        "val_batches = iter(val_dataloader)\n",
        "\n",
        "for train_sample, val_sample in zip(train_batches, val_batches):\n",
        "    print('Shape of the first training sample & corresponding mask:', train_sample[1][0].shape, train_sample[2][0].shape)\n",
        "    print('Shape of the first validation sample & corresponding mask:', val_sample[1][0].shape, val_sample[2][0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model Architecture**"
      ],
      "metadata": {
        "id": "Zu9tB-sdXj3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QaGpeaiMOgy6"
      },
      "outputs": [],
      "source": [
        "def double_convolution(in_channels, out_channels):\n",
        "    \"\"\"\n",
        "    In the original paper implementation, the convolution operations were\n",
        "    not padded but we are padding them here. This is because, we need the\n",
        "    output result size to be same as input size.\n",
        "    \"\"\"\n",
        "    conv_op = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                            nn.ReLU(inplace=True),\n",
        "\n",
        "                            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                            nn.ReLU(inplace=True)\n",
        "                           )\n",
        "    return conv_op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p2zajPGeOEMu"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Contracting path.\n",
        "\n",
        "        # Each convolution is applied twice.\n",
        "        self.down_convolution_1 = double_convolution(1, 64)\n",
        "        self.down_convolution_2 = double_convolution(64, 128)\n",
        "        self.down_convolution_3 = double_convolution(128, 256)\n",
        "        self.down_convolution_4 = double_convolution(256, 512)\n",
        "        self.down_convolution_5 = double_convolution(512, 1024)\n",
        "\n",
        "\n",
        "        # Expanding path.\n",
        "        self.up_transpose_1 = nn.ConvTranspose2d(\n",
        "                                                in_channels=1024, out_channels=512,\n",
        "                                                kernel_size=2,\n",
        "                                                stride=2)\n",
        "\n",
        "        # Below, `in_channels` again becomes 1024 as we are concatinating.\n",
        "        self.up_convolution_1 = double_convolution(1024, 512)\n",
        "\n",
        "        self.up_transpose_2 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=512, out_channels=256,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_2 = double_convolution(512, 256)\n",
        "\n",
        "        self.up_transpose_3 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=256, out_channels=128,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_3 = double_convolution(256, 128)\n",
        "\n",
        "        self.up_transpose_4 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=128, out_channels=64,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_4 = double_convolution(128, 64)\n",
        "\n",
        "        # output => `out_channels` as per the number of classes.\n",
        "        self.out = nn.Conv2d(\n",
        "                              in_channels=64, out_channels=num_classes,\n",
        "                              kernel_size=1\n",
        "                            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        down_1 = self.down_convolution_1(x)\n",
        "        down_2 = self.max_pool2d(down_1)\n",
        "        down_3 = self.down_convolution_2(down_2)\n",
        "        down_4 = self.max_pool2d(down_3)\n",
        "        down_5 = self.down_convolution_3(down_4)\n",
        "        down_6 = self.max_pool2d(down_5)\n",
        "        down_7 = self.down_convolution_4(down_6)\n",
        "        down_8 = self.max_pool2d(down_7)\n",
        "        down_9 = self.down_convolution_5(down_8)\n",
        "\n",
        "        # *** DO NOT APPLY MAX POOL TO down_9 ***\n",
        "\n",
        "        up_1 = self.up_transpose_1(down_9)\n",
        "        x = self.up_convolution_1(torch.cat([down_7, up_1], 1))\n",
        "\n",
        "        up_2 = self.up_transpose_2(x)\n",
        "        x = self.up_convolution_2(torch.cat([down_5, up_2], 1))\n",
        "\n",
        "        up_3 = self.up_transpose_3(x)\n",
        "        x = self.up_convolution_3(torch.cat([down_3, up_3], 1))\n",
        "\n",
        "        up_4 = self.up_transpose_4(x)\n",
        "        x = self.up_convolution_4(torch.cat([down_1, up_4], 1))\n",
        "\n",
        "        out = self.out(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fu4L1eT1kQ-2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(test_loader, model, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0  # Initialize the test loss\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for images_path, images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, masks)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)  # Average over all batches\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "whole_img_model = UNet(num_classes=1).to(device)\n",
        "whole_img_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/unet_lung_segmentation_0.009356894996017218.pth\", map_location=device))\n",
        "whole_img_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDrSz3HX7Euj",
        "outputId": "2d2a63b9-2c18-4e7a-da3d-1436228419d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (down_convolution_1): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_5): Sequential(\n",
              "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_1): Sequential(\n",
              "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_2): Sequential(\n",
              "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_3): Sequential(\n",
              "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_4): Sequential(\n",
              "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (out): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izisRmqypeG2"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKqHhMjTpi2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4099e7c3-5de2-4bf8-ae08-b9abf134deed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0094\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation data\n",
        "val_loss = evaluate_model(val_dataloader, whole_img_model, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detection**"
      ],
      "metadata": {
        "id": "EqNCkH7IgNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOQTxe04eWss",
        "outputId": "4ecb8361-fd4c-49a5-ed5b-c4dd94f6e6c4"
      },
      "execution_count": null,
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.143-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.143-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.143 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSkpetiOjxnr",
        "outputId": "98337861-112c-43c6-ac2b-b120130f1276"
      },
      "execution_count": null,
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_yolo_format(xmin, ymin, xmax, ymax, img_width, img_height):\n",
        "    x_center = (xmin + xmax) / 2 / img_width\n",
        "    y_center = (ymin + ymax) / 2 / img_height\n",
        "    width = (xmax - xmin) / img_width\n",
        "    height = (ymax - ymin) / img_height\n",
        "    return [0, x_center, y_center, width, height]"
      ],
      "metadata": {
        "id": "iGKDICUQB4Pv"
      },
      "execution_count": null,
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q '/content/LungTumorDetectionAndSegmentation.zip' -d '/content/dataset'"
      ],
      "metadata": {
        "id": "Pmd4Rm4MiIEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_split(path,split):\n",
        "    image_dir = os.path.join(path, split, \"images\")\n",
        "    label_dir = os.path.join(path, split, \"detections\")\n",
        "\n",
        "    out_image_dir = os.path.join(path,\"images\",split)\n",
        "    out_label_dir = os.path.join(path,\"labels\",split)\n",
        "\n",
        "    os.makedirs(out_image_dir, exist_ok=True)\n",
        "    os.makedirs(out_label_dir, exist_ok=True)\n",
        "\n",
        "    for subject in os.listdir(image_dir):\n",
        "\n",
        "        subject_image_path = os.path.join(image_dir, subject)\n",
        "        subject_label_path = os.path.join(label_dir, subject)\n",
        "\n",
        "        for img_file in os.listdir(subject_image_path):\n",
        "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(subject_image_path, img_file)\n",
        "            label_file = img_file.rsplit(\".\", 1)[0] + \".txt\"\n",
        "            label_path = os.path.join(subject_label_path, label_file)\n",
        "\n",
        "            with Image.open(img_path) as img:\n",
        "                w, h = img.size\n",
        "\n",
        "            new_img_name = f\"{subject}_{img_file}\"\n",
        "            new_img_path = os.path.join(out_image_dir, new_img_name)\n",
        "            shutil.copy(img_path, new_img_path)\n",
        "\n",
        "\n",
        "            yolo_lines = []\n",
        "            if os.path.exists(label_path):\n",
        "                with open(label_path, \"r\") as f:\n",
        "                    for line in f:\n",
        "\n",
        "                        vals = list(map(float, line.strip().replace(',', ' ').split()))\n",
        "                        if len(vals) != 4:\n",
        "                            continue\n",
        "                        xmin, ymin, xmax, ymax = vals\n",
        "                        yolo_vals = convert_to_yolo_format(xmin, ymin, xmax, ymax, w, h)\n",
        "                        yolo_lines.append(\" \".join(map(str, yolo_vals)))\n",
        "\n",
        "            out_label_path = os.path.join(out_label_dir, new_img_name.rsplit(\".\", 1)[0] + \".txt\")\n",
        "            with open(out_label_path, \"w\") as f:\n",
        "                f.write(\"\\n\".join(yolo_lines))"
      ],
      "metadata": {
        "id": "zde6mzw4etdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of dataset\n",
        "path_name='/content/dataset'\n",
        "process_split(path_name,'val')"
      ],
      "metadata": {
        "id": "anQz-aJ0ewn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lung_tumor.yaml\", \"w\") as f:\n",
        "    f.write(f\"\"\"train: {os.path.join(path_name, 'images', \"train\")}\n",
        "val: {os.path.join(path_name, 'images', \"val\")}\n",
        "nc: 1\n",
        "names: ['tumor']\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "TzVjxN_rif7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of model\n",
        "model = YOLO('/content/best(2).pt')\n",
        "metrics = model.val(data='lung_tumor.yaml', split='val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AKKA6objkdu",
        "outputId": "7bc5197e-527e-426d-b996-e6f97f87dac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.143 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1132.3Â±277.1 MB/s, size: 34.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/val.cache... 98 images, 20 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:30<00:00, 12.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         98         86      0.873      0.605      0.688      0.389\n",
            "Speed: 20.7ms preprocess, 884.3ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"loss: {metrics.speed['loss']}\")\n",
        "print(f\"mAP@0.5: {metrics.box.map50:.4f}\")\n",
        "print(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKTYD6nJjlZh",
        "outputId": "8b5f1dbf-4b4b-4f1f-afcd-28214c8eeff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.00014389795013608372\n",
            "mAP@0.5: 0.6883\n",
            "mAP@0.5:0.95: 0.3887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation on detected images (2 stages modelling)**"
      ],
      "metadata": {
        "id": "taI6gJlKVCue"
        "id": "DZKQGXv7QUs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "7GvQHDAcUOfJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                                  nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "                                  nn.BatchNorm2d(out_channels),\n",
        "                                  nn.ReLU(inplace=True),\n",
        "                                  nn.Dropout2d(0.15),  # Slightly increased dropout\n",
        "                                  nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "                                  nn.BatchNorm2d(out_channels),\n",
        "                                  nn.ReLU(inplace=True)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "9dUOCSYme3Qd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[32, 64, 128, 256]):\n",
        "        super().__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Downsampling path (encoder)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "\n",
        "        # Upsampling path (decoder)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)  # transpose conv\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            x = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](x)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "-muAjqD9e6RZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    IOU between two sets of boxes\n",
        "    :param boxes1: (Tensor of shape N x 4)\n",
        "    :param boxes2: (Tensor of shape M x 4)\n",
        "    :return: IOU matrix of shape N x M\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
        "        return torch.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=torch.float32, device=boxes1.device)\n",
        "\n",
        "    # Ensure boxes are in correct format and have valid areas\n",
        "    assert boxes1.shape[1] == 4, f\"boxes1 should have 4 coordinates, got {boxes1.shape[1]}\"\n",
        "    assert boxes2.shape[1] == 4, f\"boxes2 should have 4 coordinates, got {boxes2.shape[1]}\"\n",
        "\n",
        "    # Area of boxes (x2-x1)*(y2-y1)\n",
        "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
        "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
        "\n",
        "    # Clamp negative areas to 0 (invalid boxes)\n",
        "    area1 = area1.clamp(min=0)\n",
        "    area2 = area2.clamp(min=0)\n",
        "\n",
        "    # Get top left x1,y1 coordinate\n",
        "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
        "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
        "\n",
        "    # Get bottom right x2,y2 coordinate\n",
        "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
        "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
        "\n",
        "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
        "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    iou = intersection_area / (union + 1e-6)  # (N, M)\n",
        "    return iou"
      ],
      "metadata": {
        "id": "kEd4IGs6V-yF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
        "    \"\"\"\n",
        "    Given all anchor boxes or proposals in image and their respective\n",
        "    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n",
        "    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n",
        "    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n",
        "        Ground truth box assignments for the anchors/proposals\n",
        "    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n",
        "    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n",
        "        for all anchors/proposal boxes\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    assert ground_truth_boxes.shape == anchors_or_proposals.shape, \\\n",
        "        f\"GT boxes shape {ground_truth_boxes.shape} != anchors shape {anchors_or_proposals.shape}\"\n",
        "    assert ground_truth_boxes.shape[1] == 4, \"Boxes should have 4 coordinates\"\n",
        "\n",
        "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n",
        "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
        "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
        "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
        "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
        "\n",
        "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n",
        "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
        "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
        "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
        "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
        "\n",
        "    # Avoid division by zero for width and height\n",
        "    widths = torch.clamp(widths, min=1e-6)\n",
        "    heights = torch.clamp(heights, min=1e-6)\n",
        "    gt_widths = torch.clamp(gt_widths, min=1e-6)\n",
        "    gt_heights = torch.clamp(gt_heights, min=1e-6)\n",
        "\n",
        "    targets_dx = (gt_center_x - center_x) / widths\n",
        "    targets_dy = (gt_center_y - center_y) / heights\n",
        "    targets_dw = torch.log(gt_widths / widths)\n",
        "    targets_dh = torch.log(gt_heights / heights)\n",
        "\n",
        "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
        "    return regression_targets"
      ],
      "metadata": {
        "id": "uSsP1nNLWBlZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
        "    \"\"\"\n",
        "    Given the transformation parameter predictions for all\n",
        "    input anchors or proposals, transform them accordingly\n",
        "    to generate predicted proposals or predicted boxes\n",
        "    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4) or (num_anchors_or_proposals, 4)\n",
        "    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
        "    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4) or (num_anchors_or_proposals, 4)\n",
        "    \"\"\"\n",
        "    # Handle both 2D and 3D input tensors\n",
        "    original_shape = box_transform_pred.shape\n",
        "    if len(original_shape) == 2:\n",
        "        # (num_anchors, 4) -> (num_anchors, 1, 4)\n",
        "        box_transform_pred = box_transform_pred.unsqueeze(1)\n",
        "        squeeze_output = True\n",
        "    else:\n",
        "        # (num_anchors, num_classes, 4)\n",
        "        squeeze_output = False\n",
        "\n",
        "    box_transform_pred = box_transform_pred.reshape(box_transform_pred.size(0), -1, 4)\n",
        "\n",
        "    # Get cx, cy, w, h from x1,y1,x2,y2\n",
        "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
        "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
        "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
        "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
        "\n",
        "    # Clamp width and height to avoid division by zero\n",
        "    w = torch.clamp(w, min=1e-6)\n",
        "    h = torch.clamp(h, min=1e-6)\n",
        "\n",
        "    dx = box_transform_pred[..., 0]\n",
        "    dy = box_transform_pred[..., 1]\n",
        "    dw = box_transform_pred[..., 2]\n",
        "    dh = box_transform_pred[..., 3]\n",
        "    # dh -> (num_anchors_or_proposals, num_classes)\n",
        "\n",
        "    # Prevent sending too large values into torch.exp()\n",
        "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
        "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
        "\n",
        "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
        "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
        "    pred_w = torch.exp(dw) * w[:, None]\n",
        "    pred_h = torch.exp(dh) * h[:, None]\n",
        "    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n",
        "\n",
        "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
        "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
        "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
        "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
        "\n",
        "    pred_boxes = torch.stack((\n",
        "        pred_box_x1,\n",
        "        pred_box_y1,\n",
        "        pred_box_x2,\n",
        "        pred_box_y2),\n",
        "        dim=2)\n",
        "    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n",
        "\n",
        "    if squeeze_output:\n",
        "        pred_boxes = pred_boxes.squeeze(1)  # (num_anchors, 4)\n",
        "\n",
        "    return pred_boxes"
      ],
      "metadata": {
        "id": "nQvmLUYyWEKz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_positive_negative(labels, positive_count, total_count):\n",
        "    \"\"\"\n",
        "    Sample positive and negative proposals for training\n",
        "    :param labels: (N,) tensor of labels where 0=background, >=1=positive\n",
        "    :param positive_count: target number of positive samples\n",
        "    :param total_count: total number of samples to return\n",
        "    :return: (pos_mask, neg_mask) boolean masks for sampled indices\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    assert labels.dim() == 1, f\"Labels should be 1D tensor, got {labels.dim()}D\"\n",
        "    assert positive_count <= total_count, f\"positive_count ({positive_count}) > total_count ({total_count})\"\n",
        "    assert total_count > 0, f\"total_count should be positive, got {total_count}\"\n",
        "\n",
        "    # Sample positive and negative proposals\n",
        "    positive = torch.where(labels >= 1)[0]\n",
        "    negative = torch.where(labels == 0)[0]\n",
        "\n",
        "    # Handle edge cases\n",
        "    if positive.numel() == 0 and negative.numel() == 0:\n",
        "        # No valid samples\n",
        "        sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "        sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "        return sampled_pos_idx_mask, sampled_neg_idx_mask\n",
        "\n",
        "    # Calculate actual number of positive and negative samples\n",
        "    num_pos = min(positive.numel(), positive_count)\n",
        "    num_neg = min(negative.numel(), total_count - num_pos)\n",
        "\n",
        "    # If we don't have enough positives, increase negatives to reach total_count\n",
        "    if num_pos < positive_count:\n",
        "        num_neg = min(negative.numel(), total_count - num_pos)\n",
        "\n",
        "    # Sample indices\n",
        "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "\n",
        "    if num_pos > 0:\n",
        "        perm_positive_idxs = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n",
        "        pos_idxs = positive[perm_positive_idxs]\n",
        "        sampled_pos_idx_mask[pos_idxs] = True\n",
        "\n",
        "    if num_neg > 0:\n",
        "        perm_negative_idxs = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n",
        "        neg_idxs = negative[perm_negative_idxs]\n",
        "        sampled_neg_idx_mask[neg_idxs] = True\n",
        "\n",
        "    return sampled_pos_idx_mask, sampled_neg_idx_mask\n",
        ""
      ],
      "metadata": {
        "id": "pBszb84uWFkz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
        "    \"\"\"\n",
        "    Clamp bounding boxes to image boundaries\n",
        "    :param boxes: (..., 4) tensor of boxes in (x1, y1, x2, y2) format\n",
        "    :param image_shape: (H, W) or (..., H, W) shape of the image\n",
        "    :return: clamped boxes with same shape as input\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        return boxes\n",
        "\n",
        "    # Handle different input shapes\n",
        "    if len(image_shape) >= 2:\n",
        "        height, width = image_shape[-2:]\n",
        "    else:\n",
        "        raise ValueError(f\"image_shape should have at least 2 dimensions, got {len(image_shape)}\")\n",
        "\n",
        "    # Extract coordinates\n",
        "    boxes_x1 = boxes[..., 0]\n",
        "    boxes_y1 = boxes[..., 1]\n",
        "    boxes_x2 = boxes[..., 2]\n",
        "    boxes_y2 = boxes[..., 3]\n",
        "\n",
        "    # Clamp coordinates to image boundaries\n",
        "    # Fix: x coordinates should be clamped to [0, width-1], y to [0, height-1]\n",
        "    boxes_x1 = boxes_x1.clamp(min=0, max=width - 1)\n",
        "    boxes_x2 = boxes_x2.clamp(min=0, max=width - 1)\n",
        "    boxes_y1 = boxes_y1.clamp(min=0, max=height - 1)\n",
        "    boxes_y2 = boxes_y2.clamp(min=0, max=height - 1)\n",
        "\n",
        "    # Ensure x2 >= x1 and y2 >= y1 (valid boxes)\n",
        "    boxes_x2 = torch.max(boxes_x1, boxes_x2)\n",
        "    boxes_y2 = torch.max(boxes_y1, boxes_y2)\n",
        "\n",
        "    # Reconstruct boxes tensor\n",
        "    boxes = torch.stack((boxes_x1, boxes_y1, boxes_x2, boxes_y2), dim=-1)\n",
        "\n",
        "    return boxes"
      ],
      "metadata": {
        "id": "zrgPv033WHGL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
        "    \"\"\"\n",
        "    Transform boxes from resized image coordinates back to original image coordinates\n",
        "    :param boxes: (N, 4) tensor of boxes in (x1, y1, x2, y2) format\n",
        "    :param new_size: (H, W) size of the resized image\n",
        "    :param original_size: (H, W) size of the original image\n",
        "    :return: boxes transformed to original image coordinates\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        return boxes\n",
        "\n",
        "    # Input validation\n",
        "    assert len(new_size) == 2, f\"new_size should have 2 elements, got {len(new_size)}\"\n",
        "    assert len(original_size) == 2, f\"original_size should have 2 elements, got {len(original_size)}\"\n",
        "    assert boxes.shape[-1] == 4, f\"boxes should have 4 coordinates, got {boxes.shape[-1]}\"\n",
        "\n",
        "    # Calculate scaling ratios\n",
        "    new_height, new_width = new_size\n",
        "    orig_height, orig_width = original_size\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if new_height == 0 or new_width == 0:\n",
        "        raise ValueError(f\"new_size cannot contain zeros: {new_size}\")\n",
        "\n",
        "    ratio_height = float(orig_height) / float(new_height)\n",
        "    ratio_width = float(orig_width) / float(new_width)\n",
        "\n",
        "    # Convert ratios to tensors on the same device as boxes\n",
        "    ratio_height = torch.tensor(ratio_height, dtype=boxes.dtype, device=boxes.device)\n",
        "    ratio_width = torch.tensor(ratio_width, dtype=boxes.dtype, device=boxes.device)\n",
        "\n",
        "    # Extract coordinates\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(-1)\n",
        "\n",
        "    # Scale coordinates\n",
        "    xmin = xmin * ratio_width\n",
        "    xmax = xmax * ratio_width\n",
        "    ymin = ymin * ratio_height\n",
        "    ymax = ymax * ratio_height\n",
        "\n",
        "    # Reconstruct boxes\n",
        "    transformed_boxes = torch.stack((xmin, ymin, xmax, ymax), dim=-1)\n",
        "\n",
        "    return transformed_boxes"
      ],
      "metadata": {
        "id": "hi3q8dM9WIvI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ROIHead(nn.Module):\n",
        "    \"\"\"\n",
        "    ROI head on top of ROI pooling layer for generating\n",
        "    classification and box transformation predictions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_config, num_classes, in_channels):\n",
        "        super(ROIHead, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.roi_batch_size = model_config['roi_batch_size']\n",
        "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
        "        self.iou_threshold = model_config['roi_iou_threshold']\n",
        "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
        "        self.nms_threshold = model_config['roi_nms_threshold']\n",
        "        self.topK_detections = model_config['roi_topk_detections']\n",
        "        self.low_score_threshold = model_config['roi_score_threshold']\n",
        "        self.pool_size = model_config['roi_pool_size']\n",
        "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
        "\n",
        "        # FC layers\n",
        "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
        "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
        "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
        "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
        "\n",
        "        # Initialize weights\n",
        "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
        "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
        "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
        "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
        "\n",
        "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
        "        \"\"\"\n",
        "        Assign ground truth targets to proposals based on IoU\n",
        "        \"\"\"\n",
        "        if gt_boxes.numel() == 0 or proposals.numel() == 0:\n",
        "            # Handle empty cases\n",
        "            labels = torch.zeros(proposals.shape[0], dtype=torch.int64, device=proposals.device)\n",
        "            matched_gt_boxes = torch.zeros_like(proposals)\n",
        "            return labels, matched_gt_boxes\n",
        "\n",
        "        # Ensure inputs are 2D\n",
        "        if gt_boxes.dim() == 1:\n",
        "            gt_boxes = gt_boxes.reshape(-1, 4)\n",
        "        if proposals.dim() == 1:\n",
        "            proposals = proposals.reshape(-1, 4)\n",
        "        if gt_labels.dim() == 0:\n",
        "            gt_labels = gt_labels.unsqueeze(0)\n",
        "\n",
        "        # Get IOU matrix\n",
        "        iou_matrix = get_iou(gt_boxes, proposals)\n",
        "\n",
        "        # For each proposal, find best matching gt box\n",
        "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
        "\n",
        "        # Classify proposals\n",
        "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
        "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
        "\n",
        "        # Update indices for background and ignored proposals\n",
        "        best_match_gt_idx[background_proposals] = -1\n",
        "        best_match_gt_idx[ignored_proposals] = -2\n",
        "\n",
        "        # Get matched gt boxes (clamp to avoid negative indexing)\n",
        "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
        "\n",
        "        # Get class labels\n",
        "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
        "        labels = labels.to(dtype=torch.int64)\n",
        "\n",
        "        # Set background and ignored labels\n",
        "        labels[background_proposals] = 0  # Background class\n",
        "        labels[ignored_proposals] = -1   # Ignored\n",
        "\n",
        "        return labels, matched_gt_boxes_for_proposals\n",
        "\n",
        "    def postprocess_detections(self, cls_scores, box_transform_pred, proposals, image_shape):\n",
        "        \"\"\"\n",
        "        Post-process predictions to get final detections\n",
        "        \"\"\"\n",
        "        device = cls_scores.device\n",
        "        num_boxes, num_classes = cls_scores.shape\n",
        "\n",
        "        # Reshape box predictions: (num_boxes, num_classes, 4)\n",
        "        box_transform_pred = box_transform_pred.view(num_boxes, num_classes, 4)\n",
        "\n",
        "        # Apply softmax to classification scores\n",
        "        cls_probs = torch.softmax(cls_scores, dim=1)\n",
        "\n",
        "        all_boxes = []\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Process each class (skip background class 0)\n",
        "        for class_idx in range(1, num_classes):\n",
        "            # Get scores for this class\n",
        "            class_scores = cls_probs[:, class_idx]\n",
        "\n",
        "            # Filter by score threshold\n",
        "            score_mask = class_scores > self.low_score_threshold\n",
        "            if not score_mask.any():\n",
        "                continue\n",
        "\n",
        "            class_scores = class_scores[score_mask]\n",
        "            class_proposals = proposals[score_mask]\n",
        "            class_box_deltas = box_transform_pred[score_mask, class_idx]\n",
        "\n",
        "            # Apply box transformations\n",
        "            class_boxes = apply_regression_pred_to_anchors_or_proposals(\n",
        "                class_box_deltas.unsqueeze(1),\n",
        "                class_proposals\n",
        "            ).squeeze(1)\n",
        "\n",
        "            # Clamp boxes to image boundary\n",
        "            class_boxes = clamp_boxes_to_image_boundary(class_boxes, image_shape)\n",
        "\n",
        "            # Apply NMS\n",
        "            keep_indices = torch.ops.torchvision.nms(\n",
        "                class_boxes, class_scores, self.nms_threshold\n",
        "            )\n",
        "\n",
        "            if len(keep_indices) > 0:\n",
        "                all_boxes.append(class_boxes[keep_indices])\n",
        "                all_scores.append(class_scores[keep_indices])\n",
        "                all_labels.append(torch.full((len(keep_indices),), class_idx,\n",
        "                                           dtype=torch.int64, device=device))\n",
        "\n",
        "        if len(all_boxes) == 0:\n",
        "            # Return empty results\n",
        "            return (torch.empty((0, 4), device=device),\n",
        "                   torch.empty((0,), device=device),\n",
        "                   torch.empty((0,), dtype=torch.int64, device=device))\n",
        "\n",
        "        # Concatenate all detections\n",
        "        final_boxes = torch.cat(all_boxes, dim=0)\n",
        "        final_scores = torch.cat(all_scores, dim=0)\n",
        "        final_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Keep top K detections\n",
        "        if len(final_scores) > self.topK_detections:\n",
        "            _, top_indices = final_scores.topk(self.topK_detections)\n",
        "            final_boxes = final_boxes[top_indices]\n",
        "            final_scores = final_scores[top_indices]\n",
        "            final_labels = final_labels[top_indices]\n",
        "\n",
        "        return final_boxes, final_scores, final_labels\n",
        "\n",
        "    def forward(self, features, proposals_list, image_shapes, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass of ROI Head\n",
        "        Args:\n",
        "            features: batch of feature maps [B, C, H, W]\n",
        "            proposals_list: list of proposals for each image\n",
        "            image_shapes: list of image shapes for each image\n",
        "            targets: list of targets (for training)\n",
        "        \"\"\"\n",
        "        batch_size = len(proposals_list)\n",
        "        device = features.device\n",
        "\n",
        "        all_detections = []\n",
        "        total_roi_cls_loss = 0.0\n",
        "        total_roi_loc_loss = 0.0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Get data for this image\n",
        "            feat = features[i:i+1]  # Keep batch dimension\n",
        "            proposals = proposals_list[i]\n",
        "            image_shape = image_shapes[i] if isinstance(image_shapes, list) else image_shapes\n",
        "\n",
        "            # Skip if no proposals\n",
        "            if proposals.numel() == 0:\n",
        "                # Return empty detections\n",
        "                empty_boxes = torch.empty((0, 4), device=device)\n",
        "                empty_scores = torch.empty((0,), device=device)\n",
        "                empty_labels = torch.empty((0,), dtype=torch.int64, device=device)\n",
        "                all_detections.append({\n",
        "                    'boxes': empty_boxes,\n",
        "                    'scores': empty_scores,\n",
        "                    'labels': empty_labels\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Ensure proposals are on correct device\n",
        "            proposals = proposals.to(device)\n",
        "\n",
        "            # Training: add ground truth and assign targets\n",
        "            if self.training and targets is not None:\n",
        "                target = targets[i] if isinstance(targets, list) else targets\n",
        "\n",
        "                # Get ground truth data\n",
        "                if isinstance(target, dict):\n",
        "                    gt_boxes = target.get('bboxes', target.get('boxes', torch.empty((0, 4))))\n",
        "                    gt_labels = target.get('labels', torch.empty((0,), dtype=torch.int64))\n",
        "                else:\n",
        "                    gt_boxes = target\n",
        "                    gt_labels = torch.ones(gt_boxes.shape[0], dtype=torch.int64)  # Default to class 1\n",
        "\n",
        "                # Ensure proper shapes and devices\n",
        "                if gt_boxes.numel() > 0:\n",
        "                    gt_boxes = gt_boxes.to(device)\n",
        "                    if gt_boxes.dim() == 1:\n",
        "                        gt_boxes = gt_boxes.reshape(-1, 4)\n",
        "\n",
        "                    gt_labels = gt_labels.to(device).flatten()\n",
        "\n",
        "                    # Add ground truth to proposals\n",
        "                    proposals = torch.cat([proposals, gt_boxes], dim=0)\n",
        "\n",
        "                    # Assign targets to proposals\n",
        "                    labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(\n",
        "                        proposals, gt_boxes, gt_labels\n",
        "                    )\n",
        "\n",
        "                    # Sample positive and negative proposals\n",
        "                    sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
        "                        labels,\n",
        "                        positive_count=self.roi_pos_count,\n",
        "                        total_count=self.roi_batch_size\n",
        "                    )\n",
        "\n",
        "                    sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
        "\n",
        "                    if len(sampled_idxs) > 0:\n",
        "                        # Keep only sampled proposals\n",
        "                        proposals = proposals[sampled_idxs]\n",
        "                        labels = labels[sampled_idxs]\n",
        "                        matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
        "                        regression_targets = boxes_to_transformation_targets(\n",
        "                            matched_gt_boxes_for_proposals, proposals\n",
        "                        )\n",
        "\n",
        "            # Skip if no proposals after sampling\n",
        "            if proposals.numel() == 0:\n",
        "                empty_boxes = torch.empty((0, 4), device=device)\n",
        "                empty_scores = torch.empty((0,), device=device)\n",
        "                empty_labels = torch.empty((0,), dtype=torch.int64, device=device)\n",
        "                all_detections.append({\n",
        "                    'boxes': empty_boxes,\n",
        "                    'scores': empty_scores,\n",
        "                    'labels': empty_labels\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Calculate spatial scale for ROI pooling\n",
        "            feat_size = feat.shape[-2:]\n",
        "            if isinstance(image_shape, (list, tuple)):\n",
        "                img_h, img_w = image_shape[-2:]\n",
        "            else:\n",
        "                img_h, img_w = image_shape.shape[-2:]\n",
        "\n",
        "            spatial_scale = min(feat_size[0] / img_h, feat_size[1] / img_w)\n",
        "\n",
        "            # ROI pooling\n",
        "            proposal_roi_pool_feats = torchvision.ops.roi_pool(\n",
        "                feat,\n",
        "                [proposals],\n",
        "                output_size=self.pool_size,\n",
        "                spatial_scale=spatial_scale\n",
        "            )\n",
        "\n",
        "            # Forward through FC layers\n",
        "            proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
        "            box_fc_6 = torch.relu(self.fc6(proposal_roi_pool_feats))\n",
        "            box_fc_7 = torch.relu(self.fc7(box_fc_6))\n",
        "            cls_scores = self.cls_layer(box_fc_7)\n",
        "            box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
        "\n",
        "            # Compute losses during training\n",
        "            if self.training and targets is not None and 'labels' in locals():\n",
        "                # Classification loss\n",
        "                valid_mask = labels >= 0  # Exclude ignored samples\n",
        "                if valid_mask.sum() > 0:\n",
        "                    cls_loss = torch.nn.functional.cross_entropy(\n",
        "                        cls_scores[valid_mask],\n",
        "                        labels[valid_mask]\n",
        "                    )\n",
        "                else:\n",
        "                    cls_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                # Regression loss (only for positive samples)\n",
        "                pos_mask = labels > 0\n",
        "                if pos_mask.sum() > 0:\n",
        "                    # Get box predictions for the correct class\n",
        "                    num_boxes = box_transform_pred.shape[0]\n",
        "                    box_transform_pred_reshaped = box_transform_pred.view(num_boxes, self.num_classes, 4)\n",
        "\n",
        "                    # Select predictions for ground truth classes\n",
        "                    pos_labels = labels[pos_mask]\n",
        "                    pos_box_preds = box_transform_pred_reshaped[pos_mask, pos_labels]\n",
        "                    pos_regression_targets = regression_targets[pos_mask]\n",
        "\n",
        "                    loc_loss = torch.nn.functional.smooth_l1_loss(\n",
        "                        pos_box_preds,\n",
        "                        pos_regression_targets,\n",
        "                        beta=1.0,\n",
        "                        reduction=\"mean\"\n",
        "                    )\n",
        "                else:\n",
        "                    loc_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                total_roi_cls_loss += cls_loss\n",
        "                total_roi_loc_loss += loc_loss\n",
        "\n",
        "            # Post-process predictions for inference\n",
        "            if not self.training:\n",
        "                boxes, scores, pred_labels = self.postprocess_detections(\n",
        "                    cls_scores, box_transform_pred, proposals, image_shape\n",
        "                )\n",
        "                all_detections.append({\n",
        "                    'boxes': boxes,\n",
        "                    'scores': scores,\n",
        "                    'labels': pred_labels\n",
        "                })\n",
        "            else:\n",
        "                # During training, return raw predictions\n",
        "                all_detections.append({\n",
        "                    'cls_scores': cls_scores,\n",
        "                    'box_predictions': box_transform_pred,\n",
        "                    'proposals': proposals\n",
        "                })\n",
        "\n",
        "        # Prepare output\n",
        "        roi_output = {\n",
        "            'detections': all_detections\n",
        "        }\n",
        "\n",
        "        if self.training and targets is not None:\n",
        "            roi_output['roi_classification_loss'] = total_roi_cls_loss / batch_size\n",
        "            roi_output['roi_localization_loss'] = total_roi_loc_loss / batch_size\n",
        "\n",
        "        return roi_output\n"
      ],
      "metadata": {
        "id": "45XkW86oWKG2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, model_config, num_classes):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "        self.backbone = vgg16.features[:-1]\n",
        "        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n",
        "                                         scales=model_config['scales'],\n",
        "                                         aspect_ratios=model_config['aspect_ratios'],\n",
        "                                         model_config=model_config)\n",
        "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
        "\n",
        "        # Freeze early layers\n",
        "        for layer in self.backbone[:10]:\n",
        "            for p in layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.image_mean = [0.485, 0.456, 0.406]\n",
        "        self.image_std = [0.229, 0.224, 0.225]\n",
        "        self.min_size = model_config['min_im_size']\n",
        "        self.max_size = model_config['max_im_size']\n",
        "\n",
        "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
        "        \"\"\"\n",
        "        Normalize and resize image and corresponding bounding boxes\n",
        "        Args:\n",
        "            image: Tensor of shape (B, C, H, W) or (C, H, W)\n",
        "            bboxes: Tensor of shape (B, N, 4) or (N, 4) or None\n",
        "        Returns:\n",
        "            resized_image: Tensor of shape (B, C, H', W')\n",
        "            resized_bboxes: Tensor of shape (B, N, 4) or None\n",
        "        \"\"\"\n",
        "        dtype, device = image.dtype, image.device\n",
        "\n",
        "        # Ensure image has batch dimension\n",
        "        original_batch_dim = image.dim() == 4\n",
        "        if not original_batch_dim:\n",
        "            image = image.unsqueeze(0)\n",
        "\n",
        "        batch_size = image.shape[0]\n",
        "\n",
        "        # Normalize\n",
        "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device).view(1, 3, 1, 1)\n",
        "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device).view(1, 3, 1, 1)\n",
        "        image = (image - mean) / std\n",
        "\n",
        "        # Calculate resize scale\n",
        "        h, w = image.shape[-2:]\n",
        "        min_size_current = min(h, w)\n",
        "        max_size_current = max(h, w)\n",
        "        scale = min(self.min_size / min_size_current, self.max_size / max_size_current)\n",
        "\n",
        "        # Resize image\n",
        "        new_h = int(h * scale)\n",
        "        new_w = int(w * scale)\n",
        "\n",
        "        image = torch.nn.functional.interpolate(\n",
        "            image,\n",
        "            size=(new_h, new_w),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Resize bboxes if provided\n",
        "        resized_bboxes = None\n",
        "        if bboxes is not None and bboxes.numel() > 0:\n",
        "            # Ensure bboxes have batch dimension\n",
        "            if bboxes.dim() == 2:  # (N, 4)\n",
        "                bboxes = bboxes.unsqueeze(0)  # (1, N, 4)\n",
        "\n",
        "            # Expand to match batch size if needed\n",
        "            if bboxes.shape[0] == 1 and batch_size > 1:\n",
        "                bboxes = bboxes.expand(batch_size, -1, -1)\n",
        "\n",
        "            # Apply scaling\n",
        "            resized_bboxes = bboxes * scale\n",
        "\n",
        "            # Clamp to image boundaries\n",
        "            resized_bboxes[:, :, [0, 2]] = torch.clamp(resized_bboxes[:, :, [0, 2]], 0, new_w)\n",
        "            resized_bboxes[:, :, [1, 3]] = torch.clamp(resized_bboxes[:, :, [1, 3]], 0, new_h)\n",
        "\n",
        "        return image, resized_bboxes\n",
        "\n",
        "    def forward(self, image, target=None):\n",
        "        \"\"\"\n",
        "        Forward pass of Faster R-CNN\n",
        "        Args:\n",
        "            image: Input image tensor (B, C, H, W)\n",
        "            target: Dictionary with 'bboxes' and 'labels' (training only)\n",
        "        Returns:\n",
        "            For training: Dictionary with losses\n",
        "            For inference: Dictionary with detections\n",
        "        \"\"\"\n",
        "        if image.dim() != 4:\n",
        "            raise ValueError(f\"Expected 4D image tensor (B,C,H,W), got shape: {image.shape}\")\n",
        "\n",
        "        original_image_size = image.shape[-2:]\n",
        "\n",
        "        # Process targets for training\n",
        "        processed_target = None\n",
        "        if self.training and target is not None:\n",
        "            # Normalize and resize first\n",
        "            image, resized_bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
        "\n",
        "            # Process targets per image (RPN expects list of targets, not batched)\n",
        "            batch_size = image.shape[0]\n",
        "            processed_target = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                img_target = {}\n",
        "\n",
        "                # Extract data for this image\n",
        "                if resized_bboxes is not None:\n",
        "                    img_bboxes = resized_bboxes[i]  # Shape: (N, 4)\n",
        "                    img_labels = target['labels'][i] if 'labels' in target else None  # Shape: (N,)\n",
        "\n",
        "                    # Filter out padding/invalid boxes (assuming label 0 means padding/background)\n",
        "                    if img_labels is not None:\n",
        "                        valid_mask = img_labels > 0\n",
        "                        img_bboxes = img_bboxes[valid_mask]\n",
        "                        img_labels = img_labels[valid_mask]\n",
        "\n",
        "                    # Additional validation for box coordinates\n",
        "                    if img_bboxes.numel() > 0:\n",
        "                        valid_box_mask = (img_bboxes[:, 2] > img_bboxes[:, 0]) & \\\n",
        "                                       (img_bboxes[:, 3] > img_bboxes[:, 1])\n",
        "                        img_bboxes = img_bboxes[valid_box_mask]\n",
        "                        if img_labels is not None:\n",
        "                            img_labels = img_labels[valid_box_mask]\n",
        "\n",
        "                    img_target['bboxes'] = img_bboxes\n",
        "                    if img_labels is not None:\n",
        "                        img_target['labels'] = img_labels\n",
        "                else:\n",
        "                    img_target['bboxes'] = torch.empty((0, 4), dtype=torch.float32, device=image.device)\n",
        "                    if 'labels' in target:\n",
        "                        img_target['labels'] = torch.empty((0,), dtype=torch.long, device=image.device)\n",
        "\n",
        "                # Copy other target information\n",
        "                for key, value in target.items():\n",
        "                    if key not in ['bboxes', 'labels']:\n",
        "                        if isinstance(value, torch.Tensor) and value.dim() > 0:\n",
        "                            img_target[key] = value[i]\n",
        "                        else:\n",
        "                            img_target[key] = value\n",
        "\n",
        "                processed_target.append(img_target)\n",
        "\n",
        "            print(f\"Debug: Processed target for {len(processed_target)} images\")\n",
        "            for i, img_tgt in enumerate(processed_target):\n",
        "                if 'bboxes' in img_tgt:\n",
        "                    print(f\"  Image {i}: {img_tgt['bboxes'].shape[0]} boxes, shape: {img_tgt['bboxes'].shape}\")\n",
        "\n",
        "        else:\n",
        "            # Inference mode\n",
        "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
        "\n",
        "        try:\n",
        "            # Extract features\n",
        "            features = self.backbone(image)\n",
        "            print(f\"Debug: Features shape: {features.shape}\")\n",
        "\n",
        "            # RPN forward pass - expects list of targets for training\n",
        "            rpn_output = self.rpn(image, features, processed_target)\n",
        "            print(f\"Debug: RPN output keys: {rpn_output.keys()}\")\n",
        "\n",
        "            if 'proposals' not in rpn_output:\n",
        "                raise RuntimeError(\"RPN did not return proposals\")\n",
        "\n",
        "            proposals = rpn_output['proposals']\n",
        "            print(f\"Debug: Proposals - {len(proposals)} batches, shapes: {[p.shape for p in proposals]}\")\n",
        "\n",
        "            # Validate proposals format\n",
        "            for i, prop in enumerate(proposals):\n",
        "                if prop.dim() != 2 or prop.shape[1] != 4:\n",
        "                    raise ValueError(f\"Invalid proposal shape at batch {i}: {prop.shape}, expected (N, 4)\")\n",
        "                # Check for invalid coordinates\n",
        "                invalid_mask = (prop[:, 2] <= prop[:, 0]) | (prop[:, 3] <= prop[:, 1])\n",
        "                if invalid_mask.any():\n",
        "                    print(f\"Warning: Found {invalid_mask.sum()} invalid proposals in batch {i}\")\n",
        "                    # Fix invalid proposals\n",
        "                    prop[invalid_mask, 2] = prop[invalid_mask, 0] + 1\n",
        "                    prop[invalid_mask, 3] = prop[invalid_mask, 1] + 1\n",
        "\n",
        "            # ROI Head forward pass\n",
        "            roi_output = self.roi_head(features, proposals, image.shape[-2:], processed_target)\n",
        "            print(f\"Debug: ROI output keys: {roi_output.keys()}\")\n",
        "\n",
        "            # FIXED: Proper loss aggregation and return format\n",
        "            if self.training:\n",
        "                # Aggregate all losses\n",
        "                losses = {}\n",
        "\n",
        "                # Add RPN losses with proper tensor conversion\n",
        "                if 'rpn_classification_loss' in rpn_output:\n",
        "                    rpn_cls_loss = rpn_output['rpn_classification_loss']\n",
        "                    if not isinstance(rpn_cls_loss, torch.Tensor):\n",
        "                        rpn_cls_loss = torch.tensor(rpn_cls_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['rpn_classification_loss'] = rpn_cls_loss\n",
        "\n",
        "                if 'rpn_localization_loss' in rpn_output:\n",
        "                    rpn_loc_loss = rpn_output['rpn_localization_loss']\n",
        "                    if not isinstance(rpn_loc_loss, torch.Tensor):\n",
        "                        rpn_loc_loss = torch.tensor(rpn_loc_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['rpn_localization_loss'] = rpn_loc_loss\n",
        "\n",
        "                # Add ROI losses with proper tensor conversion\n",
        "                if 'roi_classification_loss' in roi_output:\n",
        "                    roi_cls_loss = roi_output['roi_classification_loss']\n",
        "                    if not isinstance(roi_cls_loss, torch.Tensor):\n",
        "                        roi_cls_loss = torch.tensor(roi_cls_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['roi_classification_loss'] = roi_cls_loss\n",
        "\n",
        "                if 'roi_localization_loss' in roi_output:\n",
        "                    roi_loc_loss = roi_output['roi_localization_loss']\n",
        "                    if not isinstance(roi_loc_loss, torch.Tensor):\n",
        "                        roi_loc_loss = torch.tensor(roi_loc_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['roi_localization_loss'] = roi_loc_loss\n",
        "\n",
        "                # Compute total loss\n",
        "                total_loss = torch.tensor(0.0, dtype=torch.float32, device=image.device, requires_grad=True)\n",
        "                for loss_name, loss_value in losses.items():\n",
        "                    if loss_value.requires_grad:\n",
        "                        total_loss = total_loss + loss_value\n",
        "                    else:\n",
        "                        total_loss = total_loss + loss_value.detach().requires_grad_(True)\n",
        "\n",
        "                losses['total_loss'] = total_loss\n",
        "\n",
        "                print(f\"Debug: Training losses computed: {list(losses.keys())}\")\n",
        "                return losses\n",
        "\n",
        "            else:\n",
        "                # Inference mode - return detections\n",
        "                result = roi_output.copy()\n",
        "\n",
        "                # Transform boxes back to original size\n",
        "                if 'detections' in result:\n",
        "                    # Handle detection format - assuming it's a list of detections per image\n",
        "                    detections = result['detections']\n",
        "                    if isinstance(detections, list):\n",
        "                        for i, detection in enumerate(detections):\n",
        "                            if isinstance(detection, dict) and 'boxes' in detection:\n",
        "                                if detection['boxes'] is not None and len(detection['boxes']) > 0:\n",
        "                                    detection['boxes'] = transform_boxes_to_original_size(\n",
        "                                        detection['boxes'], image.shape[-2:], original_image_size\n",
        "                                    )\n",
        "                    elif isinstance(detections, dict) and 'boxes' in detections:\n",
        "                        if detections['boxes'] is not None and len(detections['boxes']) > 0:\n",
        "                            detections['boxes'] = transform_boxes_to_original_size(\n",
        "                                detections['boxes'], image.shape[-2:], original_image_size\n",
        "                            )\n",
        "\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in FasterRCNN forward pass: {e}\")\n",
        "            print(f\"Image shape: {image.shape}\")\n",
        "            if processed_target:\n",
        "                if isinstance(processed_target, list):\n",
        "                    print(f\"Target list length: {len(processed_target)}\")\n",
        "                    for i, tgt in enumerate(processed_target):\n",
        "                        print(f\"  Target {i} keys: {tgt.keys()}\")\n",
        "                        if 'bboxes' in tgt:\n",
        "                            print(f\"  Target {i} bboxes shape: {tgt['bboxes'].shape}\")\n",
        "                else:\n",
        "                    print(f\"Target keys: {processed_target.keys()}\")\n",
        "                    if 'bboxes' in processed_target and processed_target['bboxes'] is not None:\n",
        "                        print(f\"Bboxes shape: {processed_target['bboxes'].shape}\")\n",
        "                        print(f\"Bboxes content: {processed_target['bboxes']}\")\n",
        "\n",
        "            # Additional debugging for box dimension errors\n",
        "            import traceback\n",
        "            print(\"Full traceback:\")\n",
        "            traceback.print_exc()\n",
        "            raise"
      ],
      "metadata": {
        "id": "sViwy7vhWLqL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_config(config_path):\n",
        "    \"\"\"Load YAML configuration file\"\"\"\n",
        "    with open(config_path, 'r') as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file)\n",
        "            return config\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(f\"Error loading config: {exc}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "kTgB2pNAWNWF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, config, device='cuda'):\n",
        "    \"\"\"\n",
        "    Minimal evaluation function for the trained model\n",
        "    \"\"\"\n",
        "    dataset_config = config['dataset_params']\n",
        "\n",
        "    # Load test dataset\n",
        "    test_dataset = TumorDataset(\n",
        "        split='test',\n",
        "        im_dir=dataset_config['im_test_path'],\n",
        "        ann_dir=dataset_config['ann_test_path']\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=lambda batch: batch[0]\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    num_detections = 0\n",
        "\n",
        "    for batch_data in tqdm(test_dataloader, desc='Evaluating'):\n",
        "        try:\n",
        "            if len(batch_data) == 3:\n",
        "                image, target, _ = batch_data\n",
        "            elif len(batch_data) == 2:\n",
        "                image, target = batch_data\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            image_batch = image.unsqueeze(0).float().to(device)\n",
        "\n",
        "            # Prepare target for training mode (to get loss)\n",
        "            boxes = target['boxes'].float().to(device)\n",
        "            labels = target['labels'].long().to(device)\n",
        "\n",
        "            if boxes.dim() == 1 and len(boxes) == 4:\n",
        "                boxes = boxes.unsqueeze(0)\n",
        "            if labels.dim() == 0:\n",
        "                labels = labels.unsqueeze(0)\n",
        "\n",
        "            target_batch = {\n",
        "                'bboxes': boxes.unsqueeze(0),\n",
        "                'labels': labels.unsqueeze(0)\n",
        "            }\n",
        "\n",
        "            # Set to training mode temporarily to get loss\n",
        "            model.train()\n",
        "            with torch.no_grad():\n",
        "                output = model(image_batch, target_batch)\n",
        "                if 'total_loss' in output:\n",
        "                    total_loss += output['total_loss'].item()\n",
        "                    num_batches += 1\n",
        "\n",
        "            # Set back to eval mode for inference\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                predictions = model(image_batch)\n",
        "                if 'detections' in predictions:\n",
        "                    num_detections += len(predictions['detections'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch: {e}\")\n",
        "            continue\n",
        "\n",
        "    model.eval()  # Ensure model is in eval mode at the end\n",
        "\n",
        "    if num_batches > 0:\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_detections = num_detections / len(test_dataset)\n",
        "        print(f'Evaluation Results:')\n",
        "        print(f'  Average Test Loss: {avg_loss:.4f}')\n",
        "        print(f'  Total Images: {len(test_dataset)}')\n",
        "        print(f'  Average Detections per Image: {avg_detections:.2f}')\n",
        "        return {\n",
        "            'avg_loss': avg_loss,\n",
        "            'total_images': len(test_dataset),\n",
        "            'avg_detections': avg_detections\n",
        "        }\n",
        "    else:\n",
        "        print(\"No valid batches processed!\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "8nOlox9bWfFf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/config/tumor.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "model = FasterRCNN(config['model_params'], num_classes=config['dataset_params']['num_classes'])\n",
        "model.load_state_dict(torch.load('/content/final_faster_rcnn_lung_tumor.pth', map_location=torch.device('cpu')))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "evaluate_model(model, config, device=device)"
      ],
      "metadata": {
        "id": "j33X-hFigV9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a6bdb1-d318-43a8-a1b0-6765babdbc92"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:00<00:00, 9377.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 98 images found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/98 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1482, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1482, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   1%|          | 1/98 [00:14<23:36, 14.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1007, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1007, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   2%|â–         | 2/98 [00:24<18:42, 11.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   3%|â–         | 3/98 [00:36<18:39, 11.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1051, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1051, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   4%|â–         | 4/98 [00:45<17:13, 11.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1096, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1096, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   5%|â–Œ         | 5/98 [00:54<15:52, 10.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1151, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1151, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   6%|â–Œ         | 6/98 [01:06<16:34, 10.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1237, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1237, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   7%|â–‹         | 7/98 [01:18<17:05, 11.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1421, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1421, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   8%|â–Š         | 8/98 [01:30<17:02, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1428, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1428, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   9%|â–‰         | 9/98 [01:41<16:47, 11.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1157, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1157, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  10%|â–ˆ         | 10/98 [01:51<15:51, 10.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  11%|â–ˆ         | 11/98 [02:03<16:10, 11.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1181, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1181, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  12%|â–ˆâ–        | 12/98 [02:12<15:11, 10.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1040, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1040, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  13%|â–ˆâ–        | 13/98 [02:21<14:07,  9.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1121, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1121, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  14%|â–ˆâ–        | 14/98 [02:30<13:48,  9.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1298, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1298, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  15%|â–ˆâ–Œ        | 15/98 [02:42<14:22, 10.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  16%|â–ˆâ–‹        | 16/98 [02:50<13:25,  9.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1377, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1377, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  17%|â–ˆâ–‹        | 17/98 [03:01<13:23,  9.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1324, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1324, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  18%|â–ˆâ–Š        | 18/98 [03:11<13:15,  9.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  19%|â–ˆâ–‰        | 19/98 [03:19<12:34,  9.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1261, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1261, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  20%|â–ˆâ–ˆ        | 20/98 [03:29<12:34,  9.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1361, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1361, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  21%|â–ˆâ–ˆâ–       | 21/98 [03:39<12:34,  9.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1296, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1296, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  22%|â–ˆâ–ˆâ–       | 22/98 [03:48<11:54,  9.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  23%|â–ˆâ–ˆâ–       | 23/98 [03:58<11:58,  9.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  24%|â–ˆâ–ˆâ–       | 24/98 [04:08<12:03,  9.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  26%|â–ˆâ–ˆâ–Œ       | 25/98 [04:16<11:23,  9.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1384, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1384, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  27%|â–ˆâ–ˆâ–‹       | 26/98 [04:28<12:13, 10.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1472, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1472, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  28%|â–ˆâ–ˆâ–Š       | 27/98 [04:41<12:43, 10.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1213, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1213, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  29%|â–ˆâ–ˆâ–Š       | 28/98 [04:51<12:16, 10.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  30%|â–ˆâ–ˆâ–‰       | 29/98 [04:59<11:21,  9.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1325, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1325, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  31%|â–ˆâ–ˆâ–ˆ       | 30/98 [05:11<11:48, 10.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1394, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1394, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 31/98 [05:21<11:31, 10.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1334, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1334, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 32/98 [05:32<11:48, 10.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1350, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1350, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 33/98 [05:41<10:54, 10.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1398, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1398, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 34/98 [05:51<10:46, 10.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1365, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1365, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/98 [06:01<10:36, 10.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1531, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1531, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 36/98 [06:13<10:55, 10.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1103, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1103, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 37/98 [06:22<10:16, 10.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1183, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1183, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 38/98 [06:32<10:01, 10.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1126, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1126, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 39/98 [06:40<09:26,  9.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1165, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1165, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/98 [06:50<09:15,  9.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1101, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1101, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 41/98 [07:00<09:08,  9.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1073, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1073, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/98 [07:08<08:36,  9.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1173, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1173, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/98 [07:18<08:37,  9.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1012, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1012, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/98 [07:27<08:32,  9.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1123, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1123, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/98 [07:36<08:02,  9.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1028, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1028, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 46/98 [07:45<08:02,  9.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1387, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1387, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 47/98 [07:57<08:32, 10.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1388, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1388, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 48/98 [08:09<08:51, 10.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1082, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1082, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 49/98 [08:17<08:05,  9.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1048, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1048, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/98 [08:27<07:52,  9.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1167, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1167, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 51/98 [08:37<07:39,  9.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1097, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1097, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/98 [08:53<08:53, 11.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1131, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1131, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/98 [09:01<08:01, 10.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1090, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1090, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 54/98 [09:11<07:38, 10.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1412, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1412, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/98 [09:23<07:49, 10.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1420, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1420, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 56/98 [09:34<07:45, 11.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1140, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1140, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 57/98 [09:43<07:06, 10.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1129, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1129, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 58/98 [09:53<06:47, 10.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1537, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1537, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 59/98 [10:05<07:02, 10.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/98 [10:14<06:24, 10.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/98 [10:24<06:15, 10.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/98 [10:34<06:04, 10.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1281, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1281, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/98 [10:42<05:36,  9.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1371, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1371, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 64/98 [10:53<05:32,  9.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1299, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1299, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 65/98 [11:03<05:25,  9.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1414, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1414, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 66/98 [11:11<05:03,  9.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 67/98 [11:22<05:01,  9.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1412, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1412, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 68/98 [11:32<04:55,  9.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1471, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1471, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 69/98 [11:41<04:38,  9.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1307, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1307, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 70/98 [11:50<04:30,  9.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1364, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1364, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/98 [12:01<04:24,  9.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1366, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1366, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/98 [12:09<04:07,  9.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1304, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1304, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/98 [12:19<03:59,  9.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1326, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1326, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 74/98 [12:29<03:53,  9.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1300, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1300, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 75/98 [12:38<03:36,  9.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1333, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1333, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 76/98 [12:48<03:29,  9.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1378, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1378, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 77/98 [12:58<03:26,  9.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 78/98 [13:08<03:13,  9.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1442, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1442, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 79/98 [13:19<03:13, 10.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1444, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1444, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/98 [13:31<03:13, 10.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1399, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1399, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81/98 [13:41<02:59, 10.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1356, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1356, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/98 [13:51<02:43, 10.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/98 [14:00<02:29,  9.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 84/98 [14:10<02:19,  9.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1329, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1329, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 85/98 [14:19<02:07,  9.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1321, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1321, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 86/98 [14:29<01:55,  9.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1319, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1319, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 87/98 [14:39<01:47,  9.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 88/98 [14:48<01:36,  9.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 89/98 [14:57<01:25,  9.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/98 [15:07<01:17,  9.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91/98 [15:17<01:08,  9.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1434, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1434, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/98 [15:28<01:00, 10.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1305, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1305, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/98 [15:38<00:50, 10.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1409, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1409, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 94/98 [15:48<00:40, 10.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1465, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1465, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 95/98 [16:00<00:31, 10.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1341, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1341, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 96/98 [16:09<00:20, 10.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1416, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1416, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 97/98 [16:19<00:10, 10.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1286, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1286, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [16:29<00:00, 10.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Evaluation Results:\n",
            "  Average Test Loss: 2.0960\n",
            "  Total Images: 98\n",
            "  Average Detections per Image: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
        "detection_model = YOLO('/content/best(2).pt')"
      ],
      "metadata": {
        "id": "EIxRbUbJUDt2"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_model = UNet().to(device)\n",
        "segmentation_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/best_unet_cropped.pth\", map_location=device))"
      ],
      "metadata": {
        "id": "YlnoXcQ5e9bz",
        "outputId": "43cba381-c330-41ed-8466-2f5fc5901f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = segment_detections(val_dataset, detection_model, segmentation_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-AJ3EH3T7cJ",
        "outputId": "2623238f-06cc-44b1-9af6-8740877fa5a5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/90.png: 1024x1024 (no detections), 654.6ms\n",
            "Speed: 10.5ms preprocess, 654.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/88.png: 1024x1024 1 tumor, 600.4ms\n",
            "Speed: 17.7ms preprocess, 600.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/84.png: 1024x1024 1 tumor, 584.8ms\n",
            "Speed: 15.0ms preprocess, 584.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/82.png: 1024x1024 1 tumor, 613.2ms\n",
            "Speed: 16.0ms preprocess, 613.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/79.png: 1024x1024 1 tumor, 806.1ms\n",
            "Speed: 17.0ms preprocess, 806.1ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/77.png: 1024x1024 1 tumor, 887.9ms\n",
            "Speed: 12.0ms preprocess, 887.9ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/80.png: 1024x1024 1 tumor, 890.6ms\n",
            "Speed: 19.3ms preprocess, 890.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/76.png: 1024x1024 1 tumor, 878.5ms\n",
            "Speed: 22.0ms preprocess, 878.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/83.png: 1024x1024 1 tumor, 745.8ms\n",
            "Speed: 12.3ms preprocess, 745.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/87.png: 1024x1024 1 tumor, 623.3ms\n",
            "Speed: 16.8ms preprocess, 623.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/78.png: 1024x1024 2 tumors, 606.6ms\n",
            "Speed: 16.4ms preprocess, 606.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/89.png: 1024x1024 (no detections), 611.9ms\n",
            "Speed: 14.6ms preprocess, 611.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/86.png: 1024x1024 2 tumors, 594.4ms\n",
            "Speed: 10.2ms preprocess, 594.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/81.png: 1024x1024 1 tumor, 593.6ms\n",
            "Speed: 9.3ms preprocess, 593.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/85.png: 1024x1024 1 tumor, 595.0ms\n",
            "Speed: 8.9ms preprocess, 595.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/75.png: 1024x1024 (no detections), 609.6ms\n",
            "Speed: 12.4ms preprocess, 609.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/214.png: 1024x1024 1 tumor, 614.3ms\n",
            "Speed: 11.1ms preprocess, 614.3ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/202.png: 1024x1024 2 tumors, 602.4ms\n",
            "Speed: 16.0ms preprocess, 602.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/218.png: 1024x1024 1 tumor, 604.1ms\n",
            "Speed: 10.5ms preprocess, 604.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/209.png: 1024x1024 1 tumor, 627.8ms\n",
            "Speed: 16.0ms preprocess, 627.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/208.png: 1024x1024 1 tumor, 562.4ms\n",
            "Speed: 11.6ms preprocess, 562.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/210.png: 1024x1024 1 tumor, 610.0ms\n",
            "Speed: 17.9ms preprocess, 610.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/201.png: 1024x1024 (no detections), 845.7ms\n",
            "Speed: 16.5ms preprocess, 845.7ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/217.png: 1024x1024 1 tumor, 1306.2ms\n",
            "Speed: 36.3ms preprocess, 1306.2ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/213.png: 1024x1024 2 tumors, 890.4ms\n",
            "Speed: 11.9ms preprocess, 890.4ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/203.png: 1024x1024 1 tumor, 905.1ms\n",
            "Speed: 21.0ms preprocess, 905.1ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/211.png: 1024x1024 1 tumor, 773.3ms\n",
            "Speed: 23.3ms preprocess, 773.3ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/204.png: 1024x1024 1 tumor, 609.9ms\n",
            "Speed: 13.6ms preprocess, 609.9ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/207.png: 1024x1024 1 tumor, 611.0ms\n",
            "Speed: 16.9ms preprocess, 611.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/212.png: 1024x1024 1 tumor, 606.9ms\n",
            "Speed: 8.7ms preprocess, 606.9ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/216.png: 1024x1024 1 tumor, 618.0ms\n",
            "Speed: 19.1ms preprocess, 618.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/215.png: 1024x1024 1 tumor, 593.1ms\n",
            "Speed: 14.8ms preprocess, 593.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/205.png: 1024x1024 1 tumor, 592.6ms\n",
            "Speed: 15.1ms preprocess, 592.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/206.png: 1024x1024 1 tumor, 601.6ms\n",
            "Speed: 16.7ms preprocess, 601.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/290.png: 1024x1024 1 tumor, 588.1ms\n",
            "Speed: 14.2ms preprocess, 588.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/282.png: 1024x1024 1 tumor, 595.2ms\n",
            "Speed: 9.0ms preprocess, 595.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/269.png: 1024x1024 (no detections), 603.0ms\n",
            "Speed: 14.7ms preprocess, 603.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/289.png: 1024x1024 1 tumor, 586.2ms\n",
            "Speed: 16.7ms preprocess, 586.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/273.png: 1024x1024 (no detections), 610.9ms\n",
            "Speed: 16.6ms preprocess, 610.9ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/278.png: 1024x1024 1 tumor, 550.2ms\n",
            "Speed: 10.2ms preprocess, 550.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/280.png: 1024x1024 1 tumor, 543.9ms\n",
            "Speed: 9.0ms preprocess, 543.9ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/276.png: 1024x1024 1 tumor, 901.9ms\n",
            "Speed: 16.9ms preprocess, 901.9ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/274.png: 1024x1024 1 tumor, 893.2ms\n",
            "Speed: 18.5ms preprocess, 893.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/270.png: 1024x1024 (no detections), 908.8ms\n",
            "Speed: 21.5ms preprocess, 908.8ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/275.png: 1024x1024 1 tumor, 915.8ms\n",
            "Speed: 21.0ms preprocess, 915.8ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/271.png: 1024x1024 (no detections), 637.6ms\n",
            "Speed: 21.6ms preprocess, 637.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/283.png: 1024x1024 1 tumor, 557.7ms\n",
            "Speed: 9.1ms preprocess, 557.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/284.png: 1024x1024 1 tumor, 568.1ms\n",
            "Speed: 10.1ms preprocess, 568.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/272.png: 1024x1024 (no detections), 651.6ms\n",
            "Speed: 16.4ms preprocess, 651.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/287.png: 1024x1024 1 tumor, 601.1ms\n",
            "Speed: 15.6ms preprocess, 601.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/288.png: 1024x1024 1 tumor, 607.4ms\n",
            "Speed: 8.7ms preprocess, 607.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/285.png: 1024x1024 1 tumor, 596.7ms\n",
            "Speed: 8.8ms preprocess, 596.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/277.png: 1024x1024 1 tumor, 589.0ms\n",
            "Speed: 15.4ms preprocess, 589.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/279.png: 1024x1024 1 tumor, 602.7ms\n",
            "Speed: 9.5ms preprocess, 602.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/281.png: 1024x1024 1 tumor, 618.1ms\n",
            "Speed: 15.0ms preprocess, 618.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/286.png: 1024x1024 1 tumor, 591.8ms\n",
            "Speed: 9.8ms preprocess, 591.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/294.png: 1024x1024 1 tumor, 606.5ms\n",
            "Speed: 17.4ms preprocess, 606.5ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/299.png: 1024x1024 (no detections), 575.3ms\n",
            "Speed: 9.2ms preprocess, 575.3ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/297.png: 1024x1024 1 tumor, 546.8ms\n",
            "Speed: 9.3ms preprocess, 546.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/303.png: 1024x1024 (no detections), 638.3ms\n",
            "Speed: 9.0ms preprocess, 638.3ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/301.png: 1024x1024 (no detections), 829.7ms\n",
            "Speed: 13.2ms preprocess, 829.7ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/292.png: 1024x1024 1 tumor, 825.3ms\n",
            "Speed: 11.8ms preprocess, 825.3ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/295.png: 1024x1024 1 tumor, 809.1ms\n",
            "Speed: 12.0ms preprocess, 809.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/293.png: 1024x1024 1 tumor, 832.7ms\n",
            "Speed: 18.9ms preprocess, 832.7ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/298.png: 1024x1024 (no detections), 726.0ms\n",
            "Speed: 11.9ms preprocess, 726.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/296.png: 1024x1024 1 tumor, 607.2ms\n",
            "Speed: 18.4ms preprocess, 607.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/300.png: 1024x1024 1 tumor, 587.6ms\n",
            "Speed: 16.1ms preprocess, 587.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/302.png: 1024x1024 (no detections), 624.0ms\n",
            "Speed: 9.5ms preprocess, 624.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/291.png: 1024x1024 1 tumor, 569.2ms\n",
            "Speed: 10.6ms preprocess, 569.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/53.png: 1024x1024 (no detections), 586.1ms\n",
            "Speed: 14.3ms preprocess, 586.1ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/49.png: 1024x1024 1 tumor, 614.5ms\n",
            "Speed: 16.8ms preprocess, 614.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/48.png: 1024x1024 1 tumor, 613.1ms\n",
            "Speed: 16.6ms preprocess, 613.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/51.png: 1024x1024 (no detections), 598.4ms\n",
            "Speed: 20.0ms preprocess, 598.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/54.png: 1024x1024 (no detections), 609.4ms\n",
            "Speed: 16.3ms preprocess, 609.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/46.png: 1024x1024 1 tumor, 622.1ms\n",
            "Speed: 16.6ms preprocess, 622.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/47.png: 1024x1024 (no detections), 594.3ms\n",
            "Speed: 14.7ms preprocess, 594.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/52.png: 1024x1024 (no detections), 610.8ms\n",
            "Speed: 9.5ms preprocess, 610.8ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/50.png: 1024x1024 (no detections), 545.5ms\n",
            "Speed: 9.1ms preprocess, 545.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "Average segmentation loss over dataset: 0.4751\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_loss': 2.0959586634927865, 'total_images': 98, 'avg_detections': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 26
              "0.47511487282239473"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}
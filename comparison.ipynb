{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraElwatany/Lung-TumorDetection-Segmentation/blob/main/comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1h22hs0Oic7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import shutil\n",
        "import yaml\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as T\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjhIHHVlXyCR",
        "outputId": "8132b959-ae79-459b-a280-7dde5b9f837c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_zlPVxYX5Wf"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdWWrZNadOiV"
      },
      "source": [
        "#### **DataLoading & Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4-J0tPoYTEZ"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/LungTumorDetectionAndSegmentation'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation on Whole Images**"
      ],
      "metadata": {
        "id": "ayGrAmJ7gI3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VJ0sUlQY9hh"
      },
      "outputs": [],
      "source": [
        "class LungTumorSegmentationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_path, transform=None, mask_transform=None):\n",
        "\n",
        "        self.images = []\n",
        "        for subject in os.listdir(os.path.join(root_path, 'images')):\n",
        "            subject_path = os.path.join(root_path, 'images', subject)\n",
        "            for image_file in os.listdir(subject_path):\n",
        "                self.images.append(os.path.join(subject_path, image_file))\n",
        "\n",
        "        self.masks = [img_path.replace('images', 'masks') for img_path in self.images]\n",
        "        self.transform = transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path = self.images[idx]\n",
        "        mask_path = self.masks[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"L\")  # grayscale image\n",
        "        mask = Image.open(mask_path).convert(\"L\")    # mask as single-channel\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "            mask = (mask > 0.5).float()  # Ensure binary values\n",
        "\n",
        "        return image_path, image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgDRAQseciyk"
      },
      "outputs": [],
      "source": [
        "image_transform = T.Compose([\n",
        "                              T.Resize((256, 256)),\n",
        "                              T.ToTensor(),\n",
        "                           ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAfvUkWEcEoE"
      },
      "outputs": [],
      "source": [
        "train_dataset = LungTumorSegmentationDataset(os.path.join(dataset_path, 'train'), image_transform, image_transform)\n",
        "val_dataset = LungTumorSegmentationDataset(os.path.join(dataset_path, 'val'), image_transform, image_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHQLI57Hc0ob",
        "outputId": "c74b9943-2379-4056-c28c-2660bec12fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the training data: 1832\n",
            "Length of the validation data: 98\n"
          ]
        }
      ],
      "source": [
        "print('Length of the training data:', len(train_dataset))\n",
        "print('Length of the validation data:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGvSiUXwfOc4",
        "outputId": "68329485-9c58-4845-f0af-9cf5da9ca7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the training data: 1832\n",
            "Length of the validation data: 98\n"
          ]
        }
      ],
      "source": [
        "print('Length of the training data:', len(train_dataset))\n",
        "print('Length of the validation data:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXdijYusfbTM"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYKKvIwIf7Bq",
        "outputId": "e73610c2-8a07-48ea-b5da-361298e2ca0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first training sample & corresponding mask: torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n",
            "Shape of the first validation sample & corresponding mask: torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "train_batches = iter(train_dataloader)\n",
        "val_batches = iter(val_dataloader)\n",
        "\n",
        "for train_sample, val_sample in zip(train_batches, val_batches):\n",
        "    print('Shape of the first training sample & corresponding mask:', train_sample[1][0].shape, train_sample[2][0].shape)\n",
        "    print('Shape of the first validation sample & corresponding mask:', val_sample[1][0].shape, val_sample[2][0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model Architecture**"
      ],
      "metadata": {
        "id": "Zu9tB-sdXj3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaGpeaiMOgy6"
      },
      "outputs": [],
      "source": [
        "def double_convolution(in_channels, out_channels):\n",
        "    \"\"\"\n",
        "    In the original paper implementation, the convolution operations were\n",
        "    not padded but we are padding them here. This is because, we need the\n",
        "    output result size to be same as input size.\n",
        "    \"\"\"\n",
        "    conv_op = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                            nn.ReLU(inplace=True),\n",
        "\n",
        "                            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                            nn.ReLU(inplace=True)\n",
        "                           )\n",
        "    return conv_op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2zajPGeOEMu"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Contracting path.\n",
        "\n",
        "        # Each convolution is applied twice.\n",
        "        self.down_convolution_1 = double_convolution(1, 64)\n",
        "        self.down_convolution_2 = double_convolution(64, 128)\n",
        "        self.down_convolution_3 = double_convolution(128, 256)\n",
        "        self.down_convolution_4 = double_convolution(256, 512)\n",
        "        self.down_convolution_5 = double_convolution(512, 1024)\n",
        "\n",
        "\n",
        "        # Expanding path.\n",
        "        self.up_transpose_1 = nn.ConvTranspose2d(\n",
        "                                                in_channels=1024, out_channels=512,\n",
        "                                                kernel_size=2,\n",
        "                                                stride=2)\n",
        "\n",
        "        # Below, `in_channels` again becomes 1024 as we are concatinating.\n",
        "        self.up_convolution_1 = double_convolution(1024, 512)\n",
        "\n",
        "        self.up_transpose_2 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=512, out_channels=256,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_2 = double_convolution(512, 256)\n",
        "\n",
        "        self.up_transpose_3 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=256, out_channels=128,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_3 = double_convolution(256, 128)\n",
        "\n",
        "        self.up_transpose_4 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=128, out_channels=64,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_4 = double_convolution(128, 64)\n",
        "\n",
        "        # output => `out_channels` as per the number of classes.\n",
        "        self.out = nn.Conv2d(\n",
        "                              in_channels=64, out_channels=num_classes,\n",
        "                              kernel_size=1\n",
        "                            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        down_1 = self.down_convolution_1(x)\n",
        "        down_2 = self.max_pool2d(down_1)\n",
        "        down_3 = self.down_convolution_2(down_2)\n",
        "        down_4 = self.max_pool2d(down_3)\n",
        "        down_5 = self.down_convolution_3(down_4)\n",
        "        down_6 = self.max_pool2d(down_5)\n",
        "        down_7 = self.down_convolution_4(down_6)\n",
        "        down_8 = self.max_pool2d(down_7)\n",
        "        down_9 = self.down_convolution_5(down_8)\n",
        "\n",
        "        # *** DO NOT APPLY MAX POOL TO down_9 ***\n",
        "\n",
        "        up_1 = self.up_transpose_1(down_9)\n",
        "        x = self.up_convolution_1(torch.cat([down_7, up_1], 1))\n",
        "\n",
        "        up_2 = self.up_transpose_2(x)\n",
        "        x = self.up_convolution_2(torch.cat([down_5, up_2], 1))\n",
        "\n",
        "        up_3 = self.up_transpose_3(x)\n",
        "        x = self.up_convolution_3(torch.cat([down_3, up_3], 1))\n",
        "\n",
        "        up_4 = self.up_transpose_4(x)\n",
        "        x = self.up_convolution_4(torch.cat([down_1, up_4], 1))\n",
        "\n",
        "        out = self.out(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu4L1eT1kQ-2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(test_loader, model, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0  # Initialize the test loss\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for images_path, images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, masks)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)  # Average over all batches\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "whole_img_model = UNet(num_classes=1).to(device)\n",
        "whole_img_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/unet_lung_segmentation_0.009356894996017218.pth\", map_location=device))\n",
        "whole_img_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDrSz3HX7Euj",
        "outputId": "2d2a63b9-2c18-4e7a-da3d-1436228419d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (down_convolution_1): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (down_convolution_5): Sequential(\n",
              "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_1): Sequential(\n",
              "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_2): Sequential(\n",
              "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_3): Sequential(\n",
              "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (up_transpose_4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up_convolution_4): Sequential(\n",
              "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (out): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izisRmqypeG2"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKqHhMjTpi2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4099e7c3-5de2-4bf8-ae08-b9abf134deed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0094\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation data\n",
        "val_loss = evaluate_model(val_dataloader, whole_img_model, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detection**"
      ],
      "metadata": {
        "id": "EqNCkH7IgNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOQTxe04eWss",
        "outputId": "4ecb8361-fd4c-49a5-ed5b-c4dd94f6e6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.143-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.143-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.143 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSkpetiOjxnr",
        "outputId": "98337861-112c-43c6-ac2b-b120130f1276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_yolo_format(xmin, ymin, xmax, ymax, img_width, img_height):\n",
        "    x_center = (xmin + xmax) / 2 / img_width\n",
        "    y_center = (ymin + ymax) / 2 / img_height\n",
        "    width = (xmax - xmin) / img_width\n",
        "    height = (ymax - ymin) / img_height\n",
        "    return [0, x_center, y_center, width, height]"
      ],
      "metadata": {
        "id": "iGKDICUQB4Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q '/content/LungTumorDetectionAndSegmentation.zip' -d '/content/dataset'"
      ],
      "metadata": {
        "id": "Pmd4Rm4MiIEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_split(path,split):\n",
        "    image_dir = os.path.join(path, split, \"images\")\n",
        "    label_dir = os.path.join(path, split, \"detections\")\n",
        "\n",
        "    out_image_dir = os.path.join(path,\"images\",split)\n",
        "    out_label_dir = os.path.join(path,\"labels\",split)\n",
        "\n",
        "    os.makedirs(out_image_dir, exist_ok=True)\n",
        "    os.makedirs(out_label_dir, exist_ok=True)\n",
        "\n",
        "    for subject in os.listdir(image_dir):\n",
        "\n",
        "        subject_image_path = os.path.join(image_dir, subject)\n",
        "        subject_label_path = os.path.join(label_dir, subject)\n",
        "\n",
        "        for img_file in os.listdir(subject_image_path):\n",
        "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(subject_image_path, img_file)\n",
        "            label_file = img_file.rsplit(\".\", 1)[0] + \".txt\"\n",
        "            label_path = os.path.join(subject_label_path, label_file)\n",
        "\n",
        "            with Image.open(img_path) as img:\n",
        "                w, h = img.size\n",
        "\n",
        "            new_img_name = f\"{subject}_{img_file}\"\n",
        "            new_img_path = os.path.join(out_image_dir, new_img_name)\n",
        "            shutil.copy(img_path, new_img_path)\n",
        "\n",
        "\n",
        "            yolo_lines = []\n",
        "            if os.path.exists(label_path):\n",
        "                with open(label_path, \"r\") as f:\n",
        "                    for line in f:\n",
        "\n",
        "                        vals = list(map(float, line.strip().replace(',', ' ').split()))\n",
        "                        if len(vals) != 4:\n",
        "                            continue\n",
        "                        xmin, ymin, xmax, ymax = vals\n",
        "                        yolo_vals = convert_to_yolo_format(xmin, ymin, xmax, ymax, w, h)\n",
        "                        yolo_lines.append(\" \".join(map(str, yolo_vals)))\n",
        "\n",
        "            out_label_path = os.path.join(out_label_dir, new_img_name.rsplit(\".\", 1)[0] + \".txt\")\n",
        "            with open(out_label_path, \"w\") as f:\n",
        "                f.write(\"\\n\".join(yolo_lines))"
      ],
      "metadata": {
        "id": "zde6mzw4etdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of dataset\n",
        "path_name='/content/dataset'\n",
        "process_split(path_name,'val')"
      ],
      "metadata": {
        "id": "anQz-aJ0ewn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lung_tumor.yaml\", \"w\") as f:\n",
        "    f.write(f\"\"\"train: {os.path.join(path_name, 'images', \"train\")}\n",
        "val: {os.path.join(path_name, 'images', \"val\")}\n",
        "nc: 1\n",
        "names: ['tumor']\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "TzVjxN_rif7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of model\n",
        "model = YOLO('/content/best(2).pt')\n",
        "metrics = model.val(data='lung_tumor.yaml', split='val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AKKA6objkdu",
        "outputId": "7bc5197e-527e-426d-b996-e6f97f87dac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.143 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1132.3Â±277.1 MB/s, size: 34.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/val.cache... 98 images, 20 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:30<00:00, 12.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         98         86      0.873      0.605      0.688      0.389\n",
            "Speed: 20.7ms preprocess, 884.3ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"loss: {metrics.speed['loss']}\")\n",
        "print(f\"mAP@0.5: {metrics.box.map50:.4f}\")\n",
        "print(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKTYD6nJjlZh",
        "outputId": "8b5f1dbf-4b4b-4f1f-afcd-28214c8eeff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.00014389795013608372\n",
            "mAP@0.5: 0.6883\n",
            "mAP@0.5:0.95: 0.3887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation on detected images (2 stages modelling)**"
      ],
      "metadata": {
        "id": "DZKQGXv7QUs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "7GvQHDAcUOfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                                  nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "                                  nn.BatchNorm2d(out_channels),\n",
        "                                  nn.ReLU(inplace=True),\n",
        "                                  nn.Dropout2d(0.15),  # Slightly increased dropout\n",
        "                                  nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "                                  nn.BatchNorm2d(out_channels),\n",
        "                                  nn.ReLU(inplace=True)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "9dUOCSYme3Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[32, 64, 128, 256]):\n",
        "        super().__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Downsampling path (encoder)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "\n",
        "        # Upsampling path (decoder)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)  # transpose conv\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            x = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](x)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "-muAjqD9e6RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_boxes_from_txt(txt_path):\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    with open(txt_path, 'r') as f:\n",
        "\n",
        "        for line in f:\n",
        "            xmin, ymin, xmax, ymax = map(float, line.strip().split(','))\n",
        "            boxes.append((int(xmin), int(ymin), int(xmax), int(ymax)))\n",
        "\n",
        "    return boxes"
      ],
      "metadata": {
        "id": "YJh4UQKyc8aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(boxA, boxB):\n",
        "\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = max(1, (boxA[2] - boxA[0])) * max(1, (boxA[3] - boxA[1]))\n",
        "    boxBArea = max(1, (boxB[2] - boxB[0])) * max(1, (boxB[3] - boxB[1]))\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "S0UZUOVoc-K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_detections(val_dataset, detection_model, segmentation_model, conf=0.3, iou_thresh=0.5):\n",
        "\n",
        "    segmentation_model.eval()\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    for img_path, _, _ in val_dataset:\n",
        "\n",
        "        img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "        gt_boxes_path = img_path.replace('images', 'detections').replace('.png', '.txt')\n",
        "        try:\n",
        "            gt_boxes = load_boxes_from_txt(gt_boxes_path)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "        mask_path = img_path.replace('images', 'masks')\n",
        "        mask_gt = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        results = detection_model(img_path, conf=conf)\n",
        "        pred_boxes = results[0].boxes\n",
        "\n",
        "\n",
        "        for pred_box in pred_boxes:\n",
        "\n",
        "            x1, y1, x2, y2 = map(int, pred_box.xyxy[0])\n",
        "\n",
        "            best_iou = 0\n",
        "            best_gt_box = None\n",
        "\n",
        "            for gt_box in gt_boxes:\n",
        "\n",
        "                iou = compute_iou((x1, y1, x2, y2), gt_box[:4])\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_box = gt_box\n",
        "\n",
        "            if best_iou < iou_thresh:\n",
        "                continue\n",
        "\n",
        "            crop_img = img_cv2[y1:y2, x1:x2]\n",
        "            crop_mask = mask_gt[y1:y2, x1:x2]\n",
        "\n",
        "            if crop_img.size == 0 or crop_mask.size == 0:\n",
        "                continue\n",
        "\n",
        "            crop_img_rgb = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
        "            image_pil = Image.fromarray(crop_img_rgb).resize((64, 64))\n",
        "\n",
        "            input_tensor = torch.tensor(np.transpose(np.array(image_pil).astype(np.float32) / 255.0, (2, 0, 1)))\n",
        "            input_tensor = input_tensor.unsqueeze(0).to(next(segmentation_model.parameters()).device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = segmentation_model(input_tensor)\n",
        "\n",
        "            pred_mask = output.squeeze(0)#.squeeze(0)  # shape: (H, W)\n",
        "            # if pred_mask.dim() == 2:  # [64, 64]\n",
        "            #    pred_mask = pred_mask.unsqueeze(0)\n",
        "\n",
        "            target_mask = Image.fromarray(crop_mask).resize((64, 64))\n",
        "            target_tensor = torch.tensor(np.array(target_mask).astype(np.float32) / 255.0)\n",
        "            target_tensor = target_tensor.unsqueeze(0).to(pred_mask.device)\n",
        "\n",
        "            loss = loss_fn(pred_mask, target_tensor)\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "    avg_loss = total_loss / count if count > 0 else 0\n",
        "    print(f\"Average segmentation loss over dataset: {avg_loss:.4f}\")\n",
        "\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "j33X-hFigV9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detection_model = YOLO('/content/best(2).pt')"
      ],
      "metadata": {
        "id": "EIxRbUbJUDt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_model = UNet().to(device)\n",
        "segmentation_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/best_unet_cropped.pth\", map_location=device))"
      ],
      "metadata": {
        "id": "YlnoXcQ5e9bz",
        "outputId": "43cba381-c330-41ed-8466-2f5fc5901f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = segment_detections(val_dataset, detection_model, segmentation_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-AJ3EH3T7cJ",
        "outputId": "2623238f-06cc-44b1-9af6-8740877fa5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/90.png: 1024x1024 (no detections), 654.6ms\n",
            "Speed: 10.5ms preprocess, 654.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/88.png: 1024x1024 1 tumor, 600.4ms\n",
            "Speed: 17.7ms preprocess, 600.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/84.png: 1024x1024 1 tumor, 584.8ms\n",
            "Speed: 15.0ms preprocess, 584.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/82.png: 1024x1024 1 tumor, 613.2ms\n",
            "Speed: 16.0ms preprocess, 613.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/79.png: 1024x1024 1 tumor, 806.1ms\n",
            "Speed: 17.0ms preprocess, 806.1ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/77.png: 1024x1024 1 tumor, 887.9ms\n",
            "Speed: 12.0ms preprocess, 887.9ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/80.png: 1024x1024 1 tumor, 890.6ms\n",
            "Speed: 19.3ms preprocess, 890.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/76.png: 1024x1024 1 tumor, 878.5ms\n",
            "Speed: 22.0ms preprocess, 878.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/83.png: 1024x1024 1 tumor, 745.8ms\n",
            "Speed: 12.3ms preprocess, 745.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/87.png: 1024x1024 1 tumor, 623.3ms\n",
            "Speed: 16.8ms preprocess, 623.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/78.png: 1024x1024 2 tumors, 606.6ms\n",
            "Speed: 16.4ms preprocess, 606.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/89.png: 1024x1024 (no detections), 611.9ms\n",
            "Speed: 14.6ms preprocess, 611.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/86.png: 1024x1024 2 tumors, 594.4ms\n",
            "Speed: 10.2ms preprocess, 594.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/81.png: 1024x1024 1 tumor, 593.6ms\n",
            "Speed: 9.3ms preprocess, 593.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/85.png: 1024x1024 1 tumor, 595.0ms\n",
            "Speed: 8.9ms preprocess, 595.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_59/75.png: 1024x1024 (no detections), 609.6ms\n",
            "Speed: 12.4ms preprocess, 609.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/214.png: 1024x1024 1 tumor, 614.3ms\n",
            "Speed: 11.1ms preprocess, 614.3ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/202.png: 1024x1024 2 tumors, 602.4ms\n",
            "Speed: 16.0ms preprocess, 602.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/218.png: 1024x1024 1 tumor, 604.1ms\n",
            "Speed: 10.5ms preprocess, 604.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/209.png: 1024x1024 1 tumor, 627.8ms\n",
            "Speed: 16.0ms preprocess, 627.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/208.png: 1024x1024 1 tumor, 562.4ms\n",
            "Speed: 11.6ms preprocess, 562.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/210.png: 1024x1024 1 tumor, 610.0ms\n",
            "Speed: 17.9ms preprocess, 610.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/201.png: 1024x1024 (no detections), 845.7ms\n",
            "Speed: 16.5ms preprocess, 845.7ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/217.png: 1024x1024 1 tumor, 1306.2ms\n",
            "Speed: 36.3ms preprocess, 1306.2ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/213.png: 1024x1024 2 tumors, 890.4ms\n",
            "Speed: 11.9ms preprocess, 890.4ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/203.png: 1024x1024 1 tumor, 905.1ms\n",
            "Speed: 21.0ms preprocess, 905.1ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/211.png: 1024x1024 1 tumor, 773.3ms\n",
            "Speed: 23.3ms preprocess, 773.3ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/204.png: 1024x1024 1 tumor, 609.9ms\n",
            "Speed: 13.6ms preprocess, 609.9ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/207.png: 1024x1024 1 tumor, 611.0ms\n",
            "Speed: 16.9ms preprocess, 611.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/212.png: 1024x1024 1 tumor, 606.9ms\n",
            "Speed: 8.7ms preprocess, 606.9ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/216.png: 1024x1024 1 tumor, 618.0ms\n",
            "Speed: 19.1ms preprocess, 618.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/215.png: 1024x1024 1 tumor, 593.1ms\n",
            "Speed: 14.8ms preprocess, 593.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/205.png: 1024x1024 1 tumor, 592.6ms\n",
            "Speed: 15.1ms preprocess, 592.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_57/206.png: 1024x1024 1 tumor, 601.6ms\n",
            "Speed: 16.7ms preprocess, 601.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/290.png: 1024x1024 1 tumor, 588.1ms\n",
            "Speed: 14.2ms preprocess, 588.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/282.png: 1024x1024 1 tumor, 595.2ms\n",
            "Speed: 9.0ms preprocess, 595.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/269.png: 1024x1024 (no detections), 603.0ms\n",
            "Speed: 14.7ms preprocess, 603.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/289.png: 1024x1024 1 tumor, 586.2ms\n",
            "Speed: 16.7ms preprocess, 586.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/273.png: 1024x1024 (no detections), 610.9ms\n",
            "Speed: 16.6ms preprocess, 610.9ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/278.png: 1024x1024 1 tumor, 550.2ms\n",
            "Speed: 10.2ms preprocess, 550.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/280.png: 1024x1024 1 tumor, 543.9ms\n",
            "Speed: 9.0ms preprocess, 543.9ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/276.png: 1024x1024 1 tumor, 901.9ms\n",
            "Speed: 16.9ms preprocess, 901.9ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/274.png: 1024x1024 1 tumor, 893.2ms\n",
            "Speed: 18.5ms preprocess, 893.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/270.png: 1024x1024 (no detections), 908.8ms\n",
            "Speed: 21.5ms preprocess, 908.8ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/275.png: 1024x1024 1 tumor, 915.8ms\n",
            "Speed: 21.0ms preprocess, 915.8ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/271.png: 1024x1024 (no detections), 637.6ms\n",
            "Speed: 21.6ms preprocess, 637.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/283.png: 1024x1024 1 tumor, 557.7ms\n",
            "Speed: 9.1ms preprocess, 557.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/284.png: 1024x1024 1 tumor, 568.1ms\n",
            "Speed: 10.1ms preprocess, 568.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/272.png: 1024x1024 (no detections), 651.6ms\n",
            "Speed: 16.4ms preprocess, 651.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/287.png: 1024x1024 1 tumor, 601.1ms\n",
            "Speed: 15.6ms preprocess, 601.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/288.png: 1024x1024 1 tumor, 607.4ms\n",
            "Speed: 8.7ms preprocess, 607.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/285.png: 1024x1024 1 tumor, 596.7ms\n",
            "Speed: 8.8ms preprocess, 596.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/277.png: 1024x1024 1 tumor, 589.0ms\n",
            "Speed: 15.4ms preprocess, 589.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/279.png: 1024x1024 1 tumor, 602.7ms\n",
            "Speed: 9.5ms preprocess, 602.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/281.png: 1024x1024 1 tumor, 618.1ms\n",
            "Speed: 15.0ms preprocess, 618.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/286.png: 1024x1024 1 tumor, 591.8ms\n",
            "Speed: 9.8ms preprocess, 591.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/294.png: 1024x1024 1 tumor, 606.5ms\n",
            "Speed: 17.4ms preprocess, 606.5ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/299.png: 1024x1024 (no detections), 575.3ms\n",
            "Speed: 9.2ms preprocess, 575.3ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/297.png: 1024x1024 1 tumor, 546.8ms\n",
            "Speed: 9.3ms preprocess, 546.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/303.png: 1024x1024 (no detections), 638.3ms\n",
            "Speed: 9.0ms preprocess, 638.3ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/301.png: 1024x1024 (no detections), 829.7ms\n",
            "Speed: 13.2ms preprocess, 829.7ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/292.png: 1024x1024 1 tumor, 825.3ms\n",
            "Speed: 11.8ms preprocess, 825.3ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/295.png: 1024x1024 1 tumor, 809.1ms\n",
            "Speed: 12.0ms preprocess, 809.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/293.png: 1024x1024 1 tumor, 832.7ms\n",
            "Speed: 18.9ms preprocess, 832.7ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/298.png: 1024x1024 (no detections), 726.0ms\n",
            "Speed: 11.9ms preprocess, 726.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/296.png: 1024x1024 1 tumor, 607.2ms\n",
            "Speed: 18.4ms preprocess, 607.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/300.png: 1024x1024 1 tumor, 587.6ms\n",
            "Speed: 16.1ms preprocess, 587.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/302.png: 1024x1024 (no detections), 624.0ms\n",
            "Speed: 9.5ms preprocess, 624.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_58/291.png: 1024x1024 1 tumor, 569.2ms\n",
            "Speed: 10.6ms preprocess, 569.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/53.png: 1024x1024 (no detections), 586.1ms\n",
            "Speed: 14.3ms preprocess, 586.1ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/49.png: 1024x1024 1 tumor, 614.5ms\n",
            "Speed: 16.8ms preprocess, 614.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/48.png: 1024x1024 1 tumor, 613.1ms\n",
            "Speed: 16.6ms preprocess, 613.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/51.png: 1024x1024 (no detections), 598.4ms\n",
            "Speed: 20.0ms preprocess, 598.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/54.png: 1024x1024 (no detections), 609.4ms\n",
            "Speed: 16.3ms preprocess, 609.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/46.png: 1024x1024 1 tumor, 622.1ms\n",
            "Speed: 16.6ms preprocess, 622.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/47.png: 1024x1024 (no detections), 594.3ms\n",
            "Speed: 14.7ms preprocess, 594.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/52.png: 1024x1024 (no detections), 610.8ms\n",
            "Speed: 9.5ms preprocess, 610.8ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/LungTumorDetectionAndSegmentation/val/images/Subject_60/50.png: 1024x1024 (no detections), 545.5ms\n",
            "Speed: 9.1ms preprocess, 545.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "Average segmentation loss over dataset: 0.4751\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47511487282239473"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation on detected images (2 stages modelling)**"
      ],
      "metadata": {
        "id": "rgKSDEZJShRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Models/config.zip"
      ],
      "metadata": {
        "id": "Kk4u3iDHSlNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Models/final_faster_rcnn_lung_tumor.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJmeyBmVnulA",
        "outputId": "83e22ee6-8e85-4788-9711-2c46405a3852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Models/final_faster_rcnn_lung_tumor.zip\n",
            "  inflating: final_faster_rcnn_lung_tumor.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_and_anns(im_dir, ann_dir):\n",
        "    \"\"\"\n",
        "    Load tumor detection dataset based on txt annotations (xmin ymin xmax ymax per line).\n",
        "    \"\"\"\n",
        "    im_infos = []\n",
        "    for img_path in tqdm(glob.glob(os.path.join(im_dir, '*/*.png'))):\n",
        "        im_info = {}\n",
        "        im_info['img_id'] = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        im_info['filename'] = img_path\n",
        "        im_info['subject'] = os.path.basename(os.path.dirname(img_path))\n",
        "\n",
        "        # Load image to get width and height\n",
        "        with Image.open(img_path) as img:\n",
        "            width, height = img.size\n",
        "        im_info['width'] = width\n",
        "        im_info['height'] = height\n",
        "\n",
        "        # Look for corresponding annotation .txt\n",
        "        ann_file = os.path.join(ann_dir, im_info['subject'], f\"{im_info['img_id']}.txt\")\n",
        "        detections = []\n",
        "        if os.path.exists(ann_file):\n",
        "            with open(ann_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if line:  # Skip empty lines\n",
        "                        try:\n",
        "                            # Handle both comma and space separated values\n",
        "                            coords = line.replace(',', ' ').split()\n",
        "                            if len(coords) >= 4:\n",
        "                                xmin, ymin, xmax, ymax = map(float, coords[:4])\n",
        "\n",
        "                                # Validate coordinates\n",
        "                                if xmin >= 0 and ymin >= 0 and xmax > xmin and ymax > ymin:\n",
        "                                    # Ensure coordinates are within image bounds\n",
        "                                    xmin = max(0, min(xmin, width - 1))\n",
        "                                    ymin = max(0, min(ymin, height - 1))\n",
        "                                    xmax = max(xmin + 1, min(xmax, width))\n",
        "                                    ymax = max(ymin + 1, min(ymax, height))\n",
        "\n",
        "                                    bbox = [xmin, ymin, xmax, ymax]\n",
        "                                    detections.append({'label': 1, 'bbox': bbox})  # 1 for tumor\n",
        "                        except ValueError:\n",
        "                            print(f\"Warning: Invalid annotation in {ann_file}: {line}\")\n",
        "                            continue\n",
        "\n",
        "        im_info['detections'] = detections\n",
        "        im_infos.append(im_info)\n",
        "\n",
        "    print('Total {} images found'.format(len(im_infos)))\n",
        "    return im_infos"
      ],
      "metadata": {
        "id": "tpj-PW6ep5dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TumorDataset(Dataset):\n",
        "    def __init__(self, split, im_dir, ann_dir):\n",
        "        self.split = split\n",
        "        self.im_dir = im_dir\n",
        "        self.ann_dir = ann_dir\n",
        "        self.label2idx = {'background': 0, 'tumor': 1}  # Fixed order\n",
        "        self.idx2label = {0: 'background', 1: 'tumor'}\n",
        "        self.images_info = load_images_and_anns(im_dir, ann_dir)\n",
        "\n",
        "        # Filter out images with no valid detections for training\n",
        "        if split == 'train':\n",
        "            self.images_info = [info for info in self.images_info if len(info['detections']) > 0]\n",
        "            print(f\"Filtered to {len(self.images_info)} training images with annotations\")\n",
        "\n",
        "        self.transforms = T.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_info = self.images_info[index]\n",
        "        im = Image.open(im_info['filename']).convert(\"RGB\")\n",
        "        to_flip = False\n",
        "\n",
        "        # Data augmentation for training\n",
        "        if self.split == 'train' and random.random() < 0.5:\n",
        "            to_flip = True\n",
        "            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        im_tensor = self.transforms(im)\n",
        "\n",
        "        # Get image dimensions after transform\n",
        "        _, height, width = im_tensor.shape\n",
        "\n",
        "        # Get boxes and labels\n",
        "        if len(im_info['detections']) == 0:\n",
        "            # For images with no annotations, create minimal valid tensors\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            area = torch.zeros((0,), dtype=torch.float32)\n",
        "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            boxes_list = []\n",
        "            labels_list = []\n",
        "\n",
        "            for d in im_info['detections']:\n",
        "                bbox = d['bbox']\n",
        "                # Ensure box coordinates are valid\n",
        "                x1, y1, x2, y2 = bbox\n",
        "\n",
        "                # Clamp coordinates to image bounds\n",
        "                x1 = max(0, min(x1, width - 1))\n",
        "                y1 = max(0, min(y1, height - 1))\n",
        "                x2 = max(x1 + 1, min(x2, width))\n",
        "                y2 = max(y1 + 1, min(y2, height))\n",
        "\n",
        "                # Only add if box has valid area\n",
        "                if x2 > x1 and y2 > y1:\n",
        "                    boxes_list.append([x1, y1, x2, y2])\n",
        "                    labels_list.append(d['label'])\n",
        "\n",
        "            if len(boxes_list) == 0:\n",
        "                # Fallback if all boxes are invalid\n",
        "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "                labels = torch.zeros((0,), dtype=torch.int64)\n",
        "                area = torch.zeros((0,), dtype=torch.float32)\n",
        "                iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
        "            else:\n",
        "                boxes = torch.tensor(boxes_list, dtype=torch.float32)\n",
        "                labels = torch.tensor(labels_list, dtype=torch.int64)\n",
        "\n",
        "                # Calculate area for each box\n",
        "                area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "                iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        # Apply horizontal flip to boxes if needed\n",
        "        if to_flip and boxes.numel() > 0:\n",
        "            boxes[:, [0, 2]] = width - boxes[:, [2, 0]]  # flip x1, x2\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'area': area,\n",
        "            'iscrowd': iscrowd,\n",
        "            'image_id': torch.tensor(index, dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        return im_tensor, target"
      ],
      "metadata": {
        "id": "gOy6bF8kJed2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "        self.scales = scales\n",
        "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
        "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
        "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
        "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
        "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
        "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
        "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
        "            else model_config['rpn_test_prenms_topk']\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
        "\n",
        "        # 3x3 conv layer\n",
        "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # 1x1 classification conv layer\n",
        "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
        "\n",
        "        # 1x1 regression\n",
        "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
        "\n",
        "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
        "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
        "            torch.nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def generate_anchors(self, image, feat):\n",
        "        \"\"\"Generate anchors for a single image\"\"\"\n",
        "        # Handle batch dimension - work with single image\n",
        "        if image.dim() == 4:\n",
        "            image = image[0]  # Take first image from batch\n",
        "        if feat.dim() == 4:\n",
        "            feat = feat[0]    # Take corresponding feature map\n",
        "\n",
        "        grid_h, grid_w = feat.shape[-2:]\n",
        "        image_h, image_w = image.shape[-2:]\n",
        "\n",
        "        # Calculate stride\n",
        "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
        "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
        "\n",
        "        # Ensure scales and aspect_ratios are on the same device\n",
        "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
        "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
        "\n",
        "        h_ratios = torch.sqrt(aspect_ratios)\n",
        "        w_ratios = 1 / h_ratios\n",
        "\n",
        "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
        "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
        "\n",
        "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
        "        base_anchors = base_anchors.round()\n",
        "\n",
        "        # Generate shifts\n",
        "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
        "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
        "\n",
        "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
        "        shifts_x = shifts_x.reshape(-1)\n",
        "        shifts_y = shifts_y.reshape(-1)\n",
        "\n",
        "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
        "\n",
        "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
        "        anchors = anchors.reshape(-1, 4)\n",
        "        return anchors\n",
        "\n",
        "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
        "        \"\"\"Assign targets to anchors\"\"\"\n",
        "        # Ensure gt_boxes is 2D\n",
        "        if gt_boxes.dim() == 1:\n",
        "            gt_boxes = gt_boxes.unsqueeze(0)\n",
        "\n",
        "        # Handle empty ground truth case\n",
        "        if gt_boxes.numel() == 0 or gt_boxes.shape[0] == 0:\n",
        "            labels = torch.zeros(anchors.shape[0], dtype=torch.float32, device=anchors.device)\n",
        "            matched_gt_boxes = torch.zeros_like(anchors)\n",
        "            return labels, matched_gt_boxes\n",
        "\n",
        "        # Get IOU matrix\n",
        "        iou_matrix = get_iou(gt_boxes, anchors)\n",
        "\n",
        "        # For each anchor get the gt box index with maximum overlap\n",
        "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
        "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
        "\n",
        "        # Apply thresholds\n",
        "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
        "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
        "        best_match_gt_idx[below_low_threshold] = -1\n",
        "        best_match_gt_idx[between_thresholds] = -2\n",
        "\n",
        "        # Add high quality matches\n",
        "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
        "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
        "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
        "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
        "\n",
        "        # Get matched gt boxes\n",
        "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
        "\n",
        "        # Create labels\n",
        "        labels = best_match_gt_idx >= 0\n",
        "        labels = labels.to(dtype=torch.float32)\n",
        "\n",
        "        background_anchors = best_match_gt_idx == -1\n",
        "        labels[background_anchors] = 0.0\n",
        "\n",
        "        ignored_anchors = best_match_gt_idx == -2\n",
        "        labels[ignored_anchors] = -1.0\n",
        "\n",
        "        return labels, matched_gt_boxes\n",
        "\n",
        "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
        "        \"\"\"Filter proposals using NMS and other criteria\"\"\"\n",
        "        # Handle empty proposals\n",
        "        if proposals.numel() == 0:\n",
        "            return proposals, cls_scores\n",
        "\n",
        "        # Pre NMS Filtering\n",
        "        cls_scores = cls_scores.reshape(-1)\n",
        "        cls_scores = torch.sigmoid(cls_scores)\n",
        "\n",
        "        # Handle case where we have fewer proposals than requested\n",
        "        num_proposals = min(self.rpn_prenms_topk, len(cls_scores))\n",
        "        if num_proposals == 0:\n",
        "            return proposals[:0], cls_scores[:0]\n",
        "\n",
        "        _, top_n_idx = cls_scores.topk(num_proposals)\n",
        "        cls_scores = cls_scores[top_n_idx]\n",
        "        proposals = proposals[top_n_idx]\n",
        "\n",
        "        # Clamp boxes to image boundary\n",
        "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
        "\n",
        "        # Filter small boxes\n",
        "        min_size = 16\n",
        "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
        "        keep = (ws >= min_size) & (hs >= min_size)\n",
        "        keep = torch.where(keep)[0]\n",
        "\n",
        "        if len(keep) == 0:\n",
        "            return proposals[:0], cls_scores[:0]\n",
        "\n",
        "        proposals = proposals[keep]\n",
        "        cls_scores = cls_scores[keep]\n",
        "\n",
        "        # NMS\n",
        "        if len(proposals) > 0:\n",
        "            keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
        "            keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
        "            keep_mask[keep_indices] = True\n",
        "            keep_indices = torch.where(keep_mask)[0]\n",
        "\n",
        "            # Sort by objectness\n",
        "            post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
        "\n",
        "            # Post NMS topk filtering\n",
        "            final_keep = min(self.rpn_topk, len(post_nms_keep_indices))\n",
        "            proposals = proposals[post_nms_keep_indices[:final_keep]]\n",
        "            cls_scores = cls_scores[post_nms_keep_indices[:final_keep]]\n",
        "\n",
        "        return proposals, cls_scores\n",
        "\n",
        "    def forward(self, images, features, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass of RPN\n",
        "        Args:\n",
        "            images: batch of images [B, C, H, W]\n",
        "            features: batch of feature maps [B, C, H', W']\n",
        "            targets: list of targets for each image (for training)\n",
        "        \"\"\"\n",
        "        batch_size = images.shape[0]\n",
        "        device = images.device\n",
        "\n",
        "        # Process features through RPN layers\n",
        "        rpn_feat = torch.relu(self.rpn_conv(features))\n",
        "        cls_scores = self.cls_layer(rpn_feat)\n",
        "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
        "\n",
        "        # Process each image in the batch\n",
        "        all_proposals = []\n",
        "        all_scores = []\n",
        "        total_rpn_cls_loss = 0.0\n",
        "        total_rpn_loc_loss = 0.0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Get single image data\n",
        "            image = images[i:i+1]  # Keep batch dimension for compatibility\n",
        "            feat = features[i:i+1]\n",
        "            cls_score_i = cls_scores[i:i+1]\n",
        "            box_pred_i = box_transform_pred[i:i+1]\n",
        "\n",
        "            # Generate anchors for this image\n",
        "            anchors = self.generate_anchors(image, feat)\n",
        "\n",
        "            # Reshape predictions\n",
        "            H, W = feat.shape[-2:]\n",
        "            num_anchors_per_location = cls_score_i.size(1)\n",
        "\n",
        "            # Reshape classification scores\n",
        "            cls_score_i = cls_score_i.permute(0, 2, 3, 1).reshape(-1, 1)\n",
        "\n",
        "            # Reshape box predictions\n",
        "            box_pred_i = box_pred_i.view(1, num_anchors_per_location, 4, H, W)\n",
        "            box_pred_i = box_pred_i.permute(0, 3, 4, 1, 2).reshape(-1, 4)\n",
        "\n",
        "            # Generate proposals\n",
        "            proposals = apply_regression_pred_to_anchors_or_proposals(\n",
        "                box_pred_i.detach().reshape(-1, 1, 4),\n",
        "                anchors\n",
        "            )\n",
        "            proposals = proposals.reshape(-1, 4)\n",
        "\n",
        "            # Filter proposals\n",
        "            proposals, scores = self.filter_proposals(\n",
        "                proposals,\n",
        "                cls_score_i.detach(),\n",
        "                image.shape\n",
        "            )\n",
        "\n",
        "            all_proposals.append(proposals)\n",
        "            all_scores.append(scores)\n",
        "\n",
        "            # Training losses\n",
        "            if self.training and targets is not None:\n",
        "                # Get target for this image\n",
        "                target = targets[i] if isinstance(targets, list) else targets\n",
        "\n",
        "                # Handle different target formats\n",
        "                if isinstance(target, dict):\n",
        "                    gt_boxes = target.get('bboxes', target.get('boxes', None))\n",
        "                else:\n",
        "                    gt_boxes = target\n",
        "\n",
        "                if gt_boxes is not None and gt_boxes.numel() > 0:\n",
        "                    # Ensure gt_boxes is 2D\n",
        "                    if gt_boxes.dim() == 1:\n",
        "                        gt_boxes = gt_boxes.reshape(-1, 4)\n",
        "\n",
        "                    # Assign targets to anchors\n",
        "                    labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
        "                        anchors, gt_boxes\n",
        "                    )\n",
        "\n",
        "                    # Get regression targets\n",
        "                    regression_targets = boxes_to_transformation_targets(\n",
        "                        matched_gt_boxes_for_anchors, anchors\n",
        "                    )\n",
        "\n",
        "                    # Sample positive and negative anchors\n",
        "                    sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
        "                        labels_for_anchors,\n",
        "                        positive_count=self.rpn_pos_count,\n",
        "                        total_count=self.rpn_batch_size\n",
        "                    )\n",
        "\n",
        "                    sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
        "\n",
        "                    # Compute losses\n",
        "                    if sampled_pos_idx_mask.sum() > 0:\n",
        "                        localization_loss = torch.nn.functional.smooth_l1_loss(\n",
        "                            box_pred_i[sampled_pos_idx_mask],\n",
        "                            regression_targets[sampled_pos_idx_mask],\n",
        "                            beta=1 / 9,\n",
        "                            reduction=\"sum\",\n",
        "                        ) / max(sampled_idxs.numel(), 1)\n",
        "                    else:\n",
        "                        localization_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                    if len(sampled_idxs) > 0:\n",
        "                        cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "                            cls_score_i[sampled_idxs].flatten(),\n",
        "                            labels_for_anchors[sampled_idxs].flatten()\n",
        "                        )\n",
        "                    else:\n",
        "                        cls_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                    total_rpn_cls_loss += cls_loss\n",
        "                    total_rpn_loc_loss += localization_loss\n",
        "\n",
        "        # Prepare output\n",
        "        rpn_output = {\n",
        "            'proposals': all_proposals,\n",
        "            'scores': all_scores\n",
        "        }\n",
        "\n",
        "        if self.training and targets is not None:\n",
        "            rpn_output['rpn_classification_loss'] = total_rpn_cls_loss / batch_size\n",
        "            rpn_output['rpn_localization_loss'] = total_rpn_loc_loss / batch_size\n",
        "\n",
        "        return rpn_output\n",
        "\n"
      ],
      "metadata": {
        "id": "ElNqmPRKt9pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    IOU between two sets of boxes\n",
        "    :param boxes1: (Tensor of shape N x 4)\n",
        "    :param boxes2: (Tensor of shape M x 4)\n",
        "    :return: IOU matrix of shape N x M\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
        "        return torch.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=torch.float32, device=boxes1.device)\n",
        "\n",
        "    # Ensure boxes are in correct format and have valid areas\n",
        "    assert boxes1.shape[1] == 4, f\"boxes1 should have 4 coordinates, got {boxes1.shape[1]}\"\n",
        "    assert boxes2.shape[1] == 4, f\"boxes2 should have 4 coordinates, got {boxes2.shape[1]}\"\n",
        "\n",
        "    # Area of boxes (x2-x1)*(y2-y1)\n",
        "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
        "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
        "\n",
        "    # Clamp negative areas to 0 (invalid boxes)\n",
        "    area1 = area1.clamp(min=0)\n",
        "    area2 = area2.clamp(min=0)\n",
        "\n",
        "    # Get top left x1,y1 coordinate\n",
        "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
        "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
        "\n",
        "    # Get bottom right x2,y2 coordinate\n",
        "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
        "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
        "\n",
        "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
        "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    iou = intersection_area / (union + 1e-6)  # (N, M)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
        "    \"\"\"\n",
        "    Given all anchor boxes or proposals in image and their respective\n",
        "    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n",
        "    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n",
        "    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n",
        "        Ground truth box assignments for the anchors/proposals\n",
        "    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n",
        "    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n",
        "        for all anchors/proposal boxes\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    assert ground_truth_boxes.shape == anchors_or_proposals.shape, \\\n",
        "        f\"GT boxes shape {ground_truth_boxes.shape} != anchors shape {anchors_or_proposals.shape}\"\n",
        "    assert ground_truth_boxes.shape[1] == 4, \"Boxes should have 4 coordinates\"\n",
        "\n",
        "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n",
        "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
        "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
        "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
        "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
        "\n",
        "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n",
        "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
        "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
        "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
        "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
        "\n",
        "    # Avoid division by zero for width and height\n",
        "    widths = torch.clamp(widths, min=1e-6)\n",
        "    heights = torch.clamp(heights, min=1e-6)\n",
        "    gt_widths = torch.clamp(gt_widths, min=1e-6)\n",
        "    gt_heights = torch.clamp(gt_heights, min=1e-6)\n",
        "\n",
        "    targets_dx = (gt_center_x - center_x) / widths\n",
        "    targets_dy = (gt_center_y - center_y) / heights\n",
        "    targets_dw = torch.log(gt_widths / widths)\n",
        "    targets_dh = torch.log(gt_heights / heights)\n",
        "\n",
        "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
        "    return regression_targets\n",
        "\n",
        "\n",
        "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
        "    \"\"\"\n",
        "    Given the transformation parameter predictions for all\n",
        "    input anchors or proposals, transform them accordingly\n",
        "    to generate predicted proposals or predicted boxes\n",
        "    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4) or (num_anchors_or_proposals, 4)\n",
        "    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
        "    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4) or (num_anchors_or_proposals, 4)\n",
        "    \"\"\"\n",
        "    # Handle both 2D and 3D input tensors\n",
        "    original_shape = box_transform_pred.shape\n",
        "    if len(original_shape) == 2:\n",
        "        # (num_anchors, 4) -> (num_anchors, 1, 4)\n",
        "        box_transform_pred = box_transform_pred.unsqueeze(1)\n",
        "        squeeze_output = True\n",
        "    else:\n",
        "        # (num_anchors, num_classes, 4)\n",
        "        squeeze_output = False\n",
        "\n",
        "    box_transform_pred = box_transform_pred.reshape(box_transform_pred.size(0), -1, 4)\n",
        "\n",
        "    # Get cx, cy, w, h from x1,y1,x2,y2\n",
        "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
        "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
        "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
        "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
        "\n",
        "    # Clamp width and height to avoid division by zero\n",
        "    w = torch.clamp(w, min=1e-6)\n",
        "    h = torch.clamp(h, min=1e-6)\n",
        "\n",
        "    dx = box_transform_pred[..., 0]\n",
        "    dy = box_transform_pred[..., 1]\n",
        "    dw = box_transform_pred[..., 2]\n",
        "    dh = box_transform_pred[..., 3]\n",
        "    # dh -> (num_anchors_or_proposals, num_classes)\n",
        "\n",
        "    # Prevent sending too large values into torch.exp()\n",
        "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
        "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
        "\n",
        "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
        "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
        "    pred_w = torch.exp(dw) * w[:, None]\n",
        "    pred_h = torch.exp(dh) * h[:, None]\n",
        "    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n",
        "\n",
        "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
        "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
        "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
        "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
        "\n",
        "    pred_boxes = torch.stack((\n",
        "        pred_box_x1,\n",
        "        pred_box_y1,\n",
        "        pred_box_x2,\n",
        "        pred_box_y2),\n",
        "        dim=2)\n",
        "    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n",
        "\n",
        "    if squeeze_output:\n",
        "        pred_boxes = pred_boxes.squeeze(1)  # (num_anchors, 4)\n",
        "\n",
        "    return pred_boxes\n"
      ],
      "metadata": {
        "id": "5epjHkOsua_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_positive_negative(labels, positive_count, total_count):\n",
        "    \"\"\"\n",
        "    Sample positive and negative proposals for training\n",
        "    :param labels: (N,) tensor of labels where 0=background, >=1=positive\n",
        "    :param positive_count: target number of positive samples\n",
        "    :param total_count: total number of samples to return\n",
        "    :return: (pos_mask, neg_mask) boolean masks for sampled indices\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    assert labels.dim() == 1, f\"Labels should be 1D tensor, got {labels.dim()}D\"\n",
        "    assert positive_count <= total_count, f\"positive_count ({positive_count}) > total_count ({total_count})\"\n",
        "    assert total_count > 0, f\"total_count should be positive, got {total_count}\"\n",
        "\n",
        "    # Sample positive and negative proposals\n",
        "    positive = torch.where(labels >= 1)[0]\n",
        "    negative = torch.where(labels == 0)[0]\n",
        "\n",
        "    # Handle edge cases\n",
        "    if positive.numel() == 0 and negative.numel() == 0:\n",
        "        # No valid samples\n",
        "        sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "        sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "        return sampled_pos_idx_mask, sampled_neg_idx_mask\n",
        "\n",
        "    # Calculate actual number of positive and negative samples\n",
        "    num_pos = min(positive.numel(), positive_count)\n",
        "    num_neg = min(negative.numel(), total_count - num_pos)\n",
        "\n",
        "    # If we don't have enough positives, increase negatives to reach total_count\n",
        "    if num_pos < positive_count:\n",
        "        num_neg = min(negative.numel(), total_count - num_pos)\n",
        "\n",
        "    # Sample indices\n",
        "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "\n",
        "    if num_pos > 0:\n",
        "        perm_positive_idxs = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n",
        "        pos_idxs = positive[perm_positive_idxs]\n",
        "        sampled_pos_idx_mask[pos_idxs] = True\n",
        "\n",
        "    if num_neg > 0:\n",
        "        perm_negative_idxs = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n",
        "        neg_idxs = negative[perm_negative_idxs]\n",
        "        sampled_neg_idx_mask[neg_idxs] = True\n",
        "\n",
        "    return sampled_pos_idx_mask, sampled_neg_idx_mask\n"
      ],
      "metadata": {
        "id": "SJRJspf9utlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
        "    \"\"\"\n",
        "    Clamp bounding boxes to image boundaries\n",
        "    :param boxes: (..., 4) tensor of boxes in (x1, y1, x2, y2) format\n",
        "    :param image_shape: (H, W) or (..., H, W) shape of the image\n",
        "    :return: clamped boxes with same shape as input\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        return boxes\n",
        "\n",
        "    # Handle different input shapes\n",
        "    if len(image_shape) >= 2:\n",
        "        height, width = image_shape[-2:]\n",
        "    else:\n",
        "        raise ValueError(f\"image_shape should have at least 2 dimensions, got {len(image_shape)}\")\n",
        "\n",
        "    # Extract coordinates\n",
        "    boxes_x1 = boxes[..., 0]\n",
        "    boxes_y1 = boxes[..., 1]\n",
        "    boxes_x2 = boxes[..., 2]\n",
        "    boxes_y2 = boxes[..., 3]\n",
        "\n",
        "    # Clamp coordinates to image boundaries\n",
        "    # Fix: x coordinates should be clamped to [0, width-1], y to [0, height-1]\n",
        "    boxes_x1 = boxes_x1.clamp(min=0, max=width - 1)\n",
        "    boxes_x2 = boxes_x2.clamp(min=0, max=width - 1)\n",
        "    boxes_y1 = boxes_y1.clamp(min=0, max=height - 1)\n",
        "    boxes_y2 = boxes_y2.clamp(min=0, max=height - 1)\n",
        "\n",
        "    # Ensure x2 >= x1 and y2 >= y1 (valid boxes)\n",
        "    boxes_x2 = torch.max(boxes_x1, boxes_x2)\n",
        "    boxes_y2 = torch.max(boxes_y1, boxes_y2)\n",
        "\n",
        "    # Reconstruct boxes tensor\n",
        "    boxes = torch.stack((boxes_x1, boxes_y1, boxes_x2, boxes_y2), dim=-1)\n",
        "\n",
        "    return boxes\n"
      ],
      "metadata": {
        "id": "EoJT68Cuuwso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
        "    \"\"\"\n",
        "    Transform boxes from resized image coordinates back to original image coordinates\n",
        "    :param boxes: (N, 4) tensor of boxes in (x1, y1, x2, y2) format\n",
        "    :param new_size: (H, W) size of the resized image\n",
        "    :param original_size: (H, W) size of the original image\n",
        "    :return: boxes transformed to original image coordinates\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        return boxes\n",
        "\n",
        "    # Input validation\n",
        "    assert len(new_size) == 2, f\"new_size should have 2 elements, got {len(new_size)}\"\n",
        "    assert len(original_size) == 2, f\"original_size should have 2 elements, got {len(original_size)}\"\n",
        "    assert boxes.shape[-1] == 4, f\"boxes should have 4 coordinates, got {boxes.shape[-1]}\"\n",
        "\n",
        "    # Calculate scaling ratios\n",
        "    new_height, new_width = new_size\n",
        "    orig_height, orig_width = original_size\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if new_height == 0 or new_width == 0:\n",
        "        raise ValueError(f\"new_size cannot contain zeros: {new_size}\")\n",
        "\n",
        "    ratio_height = float(orig_height) / float(new_height)\n",
        "    ratio_width = float(orig_width) / float(new_width)\n",
        "\n",
        "    # Convert ratios to tensors on the same device as boxes\n",
        "    ratio_height = torch.tensor(ratio_height, dtype=boxes.dtype, device=boxes.device)\n",
        "    ratio_width = torch.tensor(ratio_width, dtype=boxes.dtype, device=boxes.device)\n",
        "\n",
        "    # Extract coordinates\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(-1)\n",
        "\n",
        "    # Scale coordinates\n",
        "    xmin = xmin * ratio_width\n",
        "    xmax = xmax * ratio_width\n",
        "    ymin = ymin * ratio_height\n",
        "    ymax = ymax * ratio_height\n",
        "\n",
        "    # Reconstruct boxes\n",
        "    transformed_boxes = torch.stack((xmin, ymin, xmax, ymax), dim=-1)\n",
        "\n",
        "    return transformed_boxes\n"
      ],
      "metadata": {
        "id": "Sn9rD3Sru09e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ROIHead(nn.Module):\n",
        "    \"\"\"\n",
        "    ROI head on top of ROI pooling layer for generating\n",
        "    classification and box transformation predictions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_config, num_classes, in_channels):\n",
        "        super(ROIHead, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.roi_batch_size = model_config['roi_batch_size']\n",
        "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
        "        self.iou_threshold = model_config['roi_iou_threshold']\n",
        "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
        "        self.nms_threshold = model_config['roi_nms_threshold']\n",
        "        self.topK_detections = model_config['roi_topk_detections']\n",
        "        self.low_score_threshold = model_config['roi_score_threshold']\n",
        "        self.pool_size = model_config['roi_pool_size']\n",
        "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
        "\n",
        "        # FC layers\n",
        "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
        "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
        "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
        "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
        "\n",
        "        # Initialize weights\n",
        "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
        "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
        "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
        "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
        "\n",
        "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
        "        \"\"\"\n",
        "        Assign ground truth targets to proposals based on IoU\n",
        "        \"\"\"\n",
        "        if gt_boxes.numel() == 0 or proposals.numel() == 0:\n",
        "            # Handle empty cases\n",
        "            labels = torch.zeros(proposals.shape[0], dtype=torch.int64, device=proposals.device)\n",
        "            matched_gt_boxes = torch.zeros_like(proposals)\n",
        "            return labels, matched_gt_boxes\n",
        "\n",
        "        # Ensure inputs are 2D\n",
        "        if gt_boxes.dim() == 1:\n",
        "            gt_boxes = gt_boxes.reshape(-1, 4)\n",
        "        if proposals.dim() == 1:\n",
        "            proposals = proposals.reshape(-1, 4)\n",
        "        if gt_labels.dim() == 0:\n",
        "            gt_labels = gt_labels.unsqueeze(0)\n",
        "\n",
        "        # Get IOU matrix\n",
        "        iou_matrix = get_iou(gt_boxes, proposals)\n",
        "\n",
        "        # For each proposal, find best matching gt box\n",
        "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
        "\n",
        "        # Classify proposals\n",
        "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
        "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
        "\n",
        "        # Update indices for background and ignored proposals\n",
        "        best_match_gt_idx[background_proposals] = -1\n",
        "        best_match_gt_idx[ignored_proposals] = -2\n",
        "\n",
        "        # Get matched gt boxes (clamp to avoid negative indexing)\n",
        "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
        "\n",
        "        # Get class labels\n",
        "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
        "        labels = labels.to(dtype=torch.int64)\n",
        "\n",
        "        # Set background and ignored labels\n",
        "        labels[background_proposals] = 0  # Background class\n",
        "        labels[ignored_proposals] = -1   # Ignored\n",
        "\n",
        "        return labels, matched_gt_boxes_for_proposals\n",
        "\n",
        "    def postprocess_detections(self, cls_scores, box_transform_pred, proposals, image_shape):\n",
        "        \"\"\"\n",
        "        Post-process predictions to get final detections\n",
        "        \"\"\"\n",
        "        device = cls_scores.device\n",
        "        num_boxes, num_classes = cls_scores.shape\n",
        "\n",
        "        # Reshape box predictions: (num_boxes, num_classes, 4)\n",
        "        box_transform_pred = box_transform_pred.view(num_boxes, num_classes, 4)\n",
        "\n",
        "        # Apply softmax to classification scores\n",
        "        cls_probs = torch.softmax(cls_scores, dim=1)\n",
        "\n",
        "        all_boxes = []\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Process each class (skip background class 0)\n",
        "        for class_idx in range(1, num_classes):\n",
        "            # Get scores for this class\n",
        "            class_scores = cls_probs[:, class_idx]\n",
        "\n",
        "            # Filter by score threshold\n",
        "            score_mask = class_scores > self.low_score_threshold\n",
        "            if not score_mask.any():\n",
        "                continue\n",
        "\n",
        "            class_scores = class_scores[score_mask]\n",
        "            class_proposals = proposals[score_mask]\n",
        "            class_box_deltas = box_transform_pred[score_mask, class_idx]\n",
        "\n",
        "            # Apply box transformations\n",
        "            class_boxes = apply_regression_pred_to_anchors_or_proposals(\n",
        "                class_box_deltas.unsqueeze(1),\n",
        "                class_proposals\n",
        "            ).squeeze(1)\n",
        "\n",
        "            # Clamp boxes to image boundary\n",
        "            class_boxes = clamp_boxes_to_image_boundary(class_boxes, image_shape)\n",
        "\n",
        "            # Apply NMS\n",
        "            keep_indices = torch.ops.torchvision.nms(\n",
        "                class_boxes, class_scores, self.nms_threshold\n",
        "            )\n",
        "\n",
        "            if len(keep_indices) > 0:\n",
        "                all_boxes.append(class_boxes[keep_indices])\n",
        "                all_scores.append(class_scores[keep_indices])\n",
        "                all_labels.append(torch.full((len(keep_indices),), class_idx,\n",
        "                                           dtype=torch.int64, device=device))\n",
        "\n",
        "        if len(all_boxes) == 0:\n",
        "            # Return empty results\n",
        "            return (torch.empty((0, 4), device=device),\n",
        "                   torch.empty((0,), device=device),\n",
        "                   torch.empty((0,), dtype=torch.int64, device=device))\n",
        "\n",
        "        # Concatenate all detections\n",
        "        final_boxes = torch.cat(all_boxes, dim=0)\n",
        "        final_scores = torch.cat(all_scores, dim=0)\n",
        "        final_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Keep top K detections\n",
        "        if len(final_scores) > self.topK_detections:\n",
        "            _, top_indices = final_scores.topk(self.topK_detections)\n",
        "            final_boxes = final_boxes[top_indices]\n",
        "            final_scores = final_scores[top_indices]\n",
        "            final_labels = final_labels[top_indices]\n",
        "\n",
        "        return final_boxes, final_scores, final_labels\n",
        "\n",
        "    def forward(self, features, proposals_list, image_shapes, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass of ROI Head\n",
        "        Args:\n",
        "            features: batch of feature maps [B, C, H, W]\n",
        "            proposals_list: list of proposals for each image\n",
        "            image_shapes: list of image shapes for each image\n",
        "            targets: list of targets (for training)\n",
        "        \"\"\"\n",
        "        batch_size = len(proposals_list)\n",
        "        device = features.device\n",
        "\n",
        "        all_detections = []\n",
        "        total_roi_cls_loss = 0.0\n",
        "        total_roi_loc_loss = 0.0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Get data for this image\n",
        "            feat = features[i:i+1]  # Keep batch dimension\n",
        "            proposals = proposals_list[i]\n",
        "            image_shape = image_shapes[i] if isinstance(image_shapes, list) else image_shapes\n",
        "\n",
        "            # Skip if no proposals\n",
        "            if proposals.numel() == 0:\n",
        "                # Return empty detections\n",
        "                empty_boxes = torch.empty((0, 4), device=device)\n",
        "                empty_scores = torch.empty((0,), device=device)\n",
        "                empty_labels = torch.empty((0,), dtype=torch.int64, device=device)\n",
        "                all_detections.append({\n",
        "                    'boxes': empty_boxes,\n",
        "                    'scores': empty_scores,\n",
        "                    'labels': empty_labels\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Ensure proposals are on correct device\n",
        "            proposals = proposals.to(device)\n",
        "\n",
        "            # Training: add ground truth and assign targets\n",
        "            if self.training and targets is not None:\n",
        "                target = targets[i] if isinstance(targets, list) else targets\n",
        "\n",
        "                # Get ground truth data\n",
        "                if isinstance(target, dict):\n",
        "                    gt_boxes = target.get('bboxes', target.get('boxes', torch.empty((0, 4))))\n",
        "                    gt_labels = target.get('labels', torch.empty((0,), dtype=torch.int64))\n",
        "                else:\n",
        "                    gt_boxes = target\n",
        "                    gt_labels = torch.ones(gt_boxes.shape[0], dtype=torch.int64)  # Default to class 1\n",
        "\n",
        "                # Ensure proper shapes and devices\n",
        "                if gt_boxes.numel() > 0:\n",
        "                    gt_boxes = gt_boxes.to(device)\n",
        "                    if gt_boxes.dim() == 1:\n",
        "                        gt_boxes = gt_boxes.reshape(-1, 4)\n",
        "\n",
        "                    gt_labels = gt_labels.to(device).flatten()\n",
        "\n",
        "                    # Add ground truth to proposals\n",
        "                    proposals = torch.cat([proposals, gt_boxes], dim=0)\n",
        "\n",
        "                    # Assign targets to proposals\n",
        "                    labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(\n",
        "                        proposals, gt_boxes, gt_labels\n",
        "                    )\n",
        "\n",
        "                    # Sample positive and negative proposals\n",
        "                    sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
        "                        labels,\n",
        "                        positive_count=self.roi_pos_count,\n",
        "                        total_count=self.roi_batch_size\n",
        "                    )\n",
        "\n",
        "                    sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
        "\n",
        "                    if len(sampled_idxs) > 0:\n",
        "                        # Keep only sampled proposals\n",
        "                        proposals = proposals[sampled_idxs]\n",
        "                        labels = labels[sampled_idxs]\n",
        "                        matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
        "                        regression_targets = boxes_to_transformation_targets(\n",
        "                            matched_gt_boxes_for_proposals, proposals\n",
        "                        )\n",
        "\n",
        "            # Skip if no proposals after sampling\n",
        "            if proposals.numel() == 0:\n",
        "                empty_boxes = torch.empty((0, 4), device=device)\n",
        "                empty_scores = torch.empty((0,), device=device)\n",
        "                empty_labels = torch.empty((0,), dtype=torch.int64, device=device)\n",
        "                all_detections.append({\n",
        "                    'boxes': empty_boxes,\n",
        "                    'scores': empty_scores,\n",
        "                    'labels': empty_labels\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Calculate spatial scale for ROI pooling\n",
        "            feat_size = feat.shape[-2:]\n",
        "            if isinstance(image_shape, (list, tuple)):\n",
        "                img_h, img_w = image_shape[-2:]\n",
        "            else:\n",
        "                img_h, img_w = image_shape.shape[-2:]\n",
        "\n",
        "            spatial_scale = min(feat_size[0] / img_h, feat_size[1] / img_w)\n",
        "\n",
        "            # ROI pooling\n",
        "            proposal_roi_pool_feats = torchvision.ops.roi_pool(\n",
        "                feat,\n",
        "                [proposals],\n",
        "                output_size=self.pool_size,\n",
        "                spatial_scale=spatial_scale\n",
        "            )\n",
        "\n",
        "            # Forward through FC layers\n",
        "            proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
        "            box_fc_6 = torch.relu(self.fc6(proposal_roi_pool_feats))\n",
        "            box_fc_7 = torch.relu(self.fc7(box_fc_6))\n",
        "            cls_scores = self.cls_layer(box_fc_7)\n",
        "            box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
        "\n",
        "            # Compute losses during training\n",
        "            if self.training and targets is not None and 'labels' in locals():\n",
        "                # Classification loss\n",
        "                valid_mask = labels >= 0  # Exclude ignored samples\n",
        "                if valid_mask.sum() > 0:\n",
        "                    cls_loss = torch.nn.functional.cross_entropy(\n",
        "                        cls_scores[valid_mask],\n",
        "                        labels[valid_mask]\n",
        "                    )\n",
        "                else:\n",
        "                    cls_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                # Regression loss (only for positive samples)\n",
        "                pos_mask = labels > 0\n",
        "                if pos_mask.sum() > 0:\n",
        "                    # Get box predictions for the correct class\n",
        "                    num_boxes = box_transform_pred.shape[0]\n",
        "                    box_transform_pred_reshaped = box_transform_pred.view(num_boxes, self.num_classes, 4)\n",
        "\n",
        "                    # Select predictions for ground truth classes\n",
        "                    pos_labels = labels[pos_mask]\n",
        "                    pos_box_preds = box_transform_pred_reshaped[pos_mask, pos_labels]\n",
        "                    pos_regression_targets = regression_targets[pos_mask]\n",
        "\n",
        "                    loc_loss = torch.nn.functional.smooth_l1_loss(\n",
        "                        pos_box_preds,\n",
        "                        pos_regression_targets,\n",
        "                        beta=1.0,\n",
        "                        reduction=\"mean\"\n",
        "                    )\n",
        "                else:\n",
        "                    loc_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                total_roi_cls_loss += cls_loss\n",
        "                total_roi_loc_loss += loc_loss\n",
        "\n",
        "            # Post-process predictions for inference\n",
        "            if not self.training:\n",
        "                boxes, scores, pred_labels = self.postprocess_detections(\n",
        "                    cls_scores, box_transform_pred, proposals, image_shape\n",
        "                )\n",
        "                all_detections.append({\n",
        "                    'boxes': boxes,\n",
        "                    'scores': scores,\n",
        "                    'labels': pred_labels\n",
        "                })\n",
        "            else:\n",
        "                # During training, return raw predictions\n",
        "                all_detections.append({\n",
        "                    'cls_scores': cls_scores,\n",
        "                    'box_predictions': box_transform_pred,\n",
        "                    'proposals': proposals\n",
        "                })\n",
        "\n",
        "        # Prepare output\n",
        "        roi_output = {\n",
        "            'detections': all_detections\n",
        "        }\n",
        "\n",
        "        if self.training and targets is not None:\n",
        "            roi_output['roi_classification_loss'] = total_roi_cls_loss / batch_size\n",
        "            roi_output['roi_localization_loss'] = total_roi_loc_loss / batch_size\n",
        "\n",
        "        return roi_output\n"
      ],
      "metadata": {
        "id": "FbchQjyYuIAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, model_config, num_classes):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "        self.backbone = vgg16.features[:-1]\n",
        "        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n",
        "                                         scales=model_config['scales'],\n",
        "                                         aspect_ratios=model_config['aspect_ratios'],\n",
        "                                         model_config=model_config)\n",
        "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
        "\n",
        "        # Freeze early layers\n",
        "        for layer in self.backbone[:10]:\n",
        "            for p in layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.image_mean = [0.485, 0.456, 0.406]\n",
        "        self.image_std = [0.229, 0.224, 0.225]\n",
        "        self.min_size = model_config['min_im_size']\n",
        "        self.max_size = model_config['max_im_size']\n",
        "\n",
        "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
        "        \"\"\"\n",
        "        Normalize and resize image and corresponding bounding boxes\n",
        "        Args:\n",
        "            image: Tensor of shape (B, C, H, W) or (C, H, W)\n",
        "            bboxes: Tensor of shape (B, N, 4) or (N, 4) or None\n",
        "        Returns:\n",
        "            resized_image: Tensor of shape (B, C, H', W')\n",
        "            resized_bboxes: Tensor of shape (B, N, 4) or None\n",
        "        \"\"\"\n",
        "        dtype, device = image.dtype, image.device\n",
        "\n",
        "        # Ensure image has batch dimension\n",
        "        original_batch_dim = image.dim() == 4\n",
        "        if not original_batch_dim:\n",
        "            image = image.unsqueeze(0)\n",
        "\n",
        "        batch_size = image.shape[0]\n",
        "\n",
        "        # Normalize\n",
        "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device).view(1, 3, 1, 1)\n",
        "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device).view(1, 3, 1, 1)\n",
        "        image = (image - mean) / std\n",
        "\n",
        "        # Calculate resize scale\n",
        "        h, w = image.shape[-2:]\n",
        "        min_size_current = min(h, w)\n",
        "        max_size_current = max(h, w)\n",
        "        scale = min(self.min_size / min_size_current, self.max_size / max_size_current)\n",
        "\n",
        "        # Resize image\n",
        "        new_h = int(h * scale)\n",
        "        new_w = int(w * scale)\n",
        "\n",
        "        image = torch.nn.functional.interpolate(\n",
        "            image,\n",
        "            size=(new_h, new_w),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Resize bboxes if provided\n",
        "        resized_bboxes = None\n",
        "        if bboxes is not None and bboxes.numel() > 0:\n",
        "            # Ensure bboxes have batch dimension\n",
        "            if bboxes.dim() == 2:  # (N, 4)\n",
        "                bboxes = bboxes.unsqueeze(0)  # (1, N, 4)\n",
        "\n",
        "            # Expand to match batch size if needed\n",
        "            if bboxes.shape[0] == 1 and batch_size > 1:\n",
        "                bboxes = bboxes.expand(batch_size, -1, -1)\n",
        "\n",
        "            # Apply scaling\n",
        "            resized_bboxes = bboxes * scale\n",
        "\n",
        "            # Clamp to image boundaries\n",
        "            resized_bboxes[:, :, [0, 2]] = torch.clamp(resized_bboxes[:, :, [0, 2]], 0, new_w)\n",
        "            resized_bboxes[:, :, [1, 3]] = torch.clamp(resized_bboxes[:, :, [1, 3]], 0, new_h)\n",
        "\n",
        "        return image, resized_bboxes\n",
        "\n",
        "    def forward(self, image, target=None):\n",
        "        \"\"\"\n",
        "        Forward pass of Faster R-CNN\n",
        "        Args:\n",
        "            image: Input image tensor (B, C, H, W)\n",
        "            target: Dictionary with 'bboxes' and 'labels' (training only)\n",
        "        Returns:\n",
        "            For training: Dictionary with losses\n",
        "            For inference: Dictionary with detections\n",
        "        \"\"\"\n",
        "        if image.dim() != 4:\n",
        "            raise ValueError(f\"Expected 4D image tensor (B,C,H,W), got shape: {image.shape}\")\n",
        "\n",
        "        original_image_size = image.shape[-2:]\n",
        "\n",
        "        # Process targets for training\n",
        "        processed_target = None\n",
        "        if self.training and target is not None:\n",
        "            # Normalize and resize first\n",
        "            image, resized_bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
        "\n",
        "            # Process targets per image (RPN expects list of targets, not batched)\n",
        "            batch_size = image.shape[0]\n",
        "            processed_target = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                img_target = {}\n",
        "\n",
        "                # Extract data for this image\n",
        "                if resized_bboxes is not None:\n",
        "                    img_bboxes = resized_bboxes[i]  # Shape: (N, 4)\n",
        "                    img_labels = target['labels'][i] if 'labels' in target else None  # Shape: (N,)\n",
        "\n",
        "                    # Filter out padding/invalid boxes (assuming label 0 means padding/background)\n",
        "                    if img_labels is not None:\n",
        "                        valid_mask = img_labels > 0\n",
        "                        img_bboxes = img_bboxes[valid_mask]\n",
        "                        img_labels = img_labels[valid_mask]\n",
        "\n",
        "                    # Additional validation for box coordinates\n",
        "                    if img_bboxes.numel() > 0:\n",
        "                        valid_box_mask = (img_bboxes[:, 2] > img_bboxes[:, 0]) & \\\n",
        "                                       (img_bboxes[:, 3] > img_bboxes[:, 1])\n",
        "                        img_bboxes = img_bboxes[valid_box_mask]\n",
        "                        if img_labels is not None:\n",
        "                            img_labels = img_labels[valid_box_mask]\n",
        "\n",
        "                    img_target['bboxes'] = img_bboxes\n",
        "                    if img_labels is not None:\n",
        "                        img_target['labels'] = img_labels\n",
        "                else:\n",
        "                    img_target['bboxes'] = torch.empty((0, 4), dtype=torch.float32, device=image.device)\n",
        "                    if 'labels' in target:\n",
        "                        img_target['labels'] = torch.empty((0,), dtype=torch.long, device=image.device)\n",
        "\n",
        "                # Copy other target information\n",
        "                for key, value in target.items():\n",
        "                    if key not in ['bboxes', 'labels']:\n",
        "                        if isinstance(value, torch.Tensor) and value.dim() > 0:\n",
        "                            img_target[key] = value[i]\n",
        "                        else:\n",
        "                            img_target[key] = value\n",
        "\n",
        "                processed_target.append(img_target)\n",
        "\n",
        "            print(f\"Debug: Processed target for {len(processed_target)} images\")\n",
        "            for i, img_tgt in enumerate(processed_target):\n",
        "                if 'bboxes' in img_tgt:\n",
        "                    print(f\"  Image {i}: {img_tgt['bboxes'].shape[0]} boxes, shape: {img_tgt['bboxes'].shape}\")\n",
        "\n",
        "        else:\n",
        "            # Inference mode\n",
        "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
        "\n",
        "        try:\n",
        "            # Extract features\n",
        "            features = self.backbone(image)\n",
        "            print(f\"Debug: Features shape: {features.shape}\")\n",
        "\n",
        "            # RPN forward pass - expects list of targets for training\n",
        "            rpn_output = self.rpn(image, features, processed_target)\n",
        "            print(f\"Debug: RPN output keys: {rpn_output.keys()}\")\n",
        "\n",
        "            if 'proposals' not in rpn_output:\n",
        "                raise RuntimeError(\"RPN did not return proposals\")\n",
        "\n",
        "            proposals = rpn_output['proposals']\n",
        "            print(f\"Debug: Proposals - {len(proposals)} batches, shapes: {[p.shape for p in proposals]}\")\n",
        "\n",
        "            # Validate proposals format\n",
        "            for i, prop in enumerate(proposals):\n",
        "                if prop.dim() != 2 or prop.shape[1] != 4:\n",
        "                    raise ValueError(f\"Invalid proposal shape at batch {i}: {prop.shape}, expected (N, 4)\")\n",
        "                # Check for invalid coordinates\n",
        "                invalid_mask = (prop[:, 2] <= prop[:, 0]) | (prop[:, 3] <= prop[:, 1])\n",
        "                if invalid_mask.any():\n",
        "                    print(f\"Warning: Found {invalid_mask.sum()} invalid proposals in batch {i}\")\n",
        "                    # Fix invalid proposals\n",
        "                    prop[invalid_mask, 2] = prop[invalid_mask, 0] + 1\n",
        "                    prop[invalid_mask, 3] = prop[invalid_mask, 1] + 1\n",
        "\n",
        "            # ROI Head forward pass\n",
        "            roi_output = self.roi_head(features, proposals, image.shape[-2:], processed_target)\n",
        "            print(f\"Debug: ROI output keys: {roi_output.keys()}\")\n",
        "\n",
        "            # FIXED: Proper loss aggregation and return format\n",
        "            if self.training:\n",
        "                # Aggregate all losses\n",
        "                losses = {}\n",
        "\n",
        "                # Add RPN losses with proper tensor conversion\n",
        "                if 'rpn_classification_loss' in rpn_output:\n",
        "                    rpn_cls_loss = rpn_output['rpn_classification_loss']\n",
        "                    if not isinstance(rpn_cls_loss, torch.Tensor):\n",
        "                        rpn_cls_loss = torch.tensor(rpn_cls_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['rpn_classification_loss'] = rpn_cls_loss\n",
        "\n",
        "                if 'rpn_localization_loss' in rpn_output:\n",
        "                    rpn_loc_loss = rpn_output['rpn_localization_loss']\n",
        "                    if not isinstance(rpn_loc_loss, torch.Tensor):\n",
        "                        rpn_loc_loss = torch.tensor(rpn_loc_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['rpn_localization_loss'] = rpn_loc_loss\n",
        "\n",
        "                # Add ROI losses with proper tensor conversion\n",
        "                if 'roi_classification_loss' in roi_output:\n",
        "                    roi_cls_loss = roi_output['roi_classification_loss']\n",
        "                    if not isinstance(roi_cls_loss, torch.Tensor):\n",
        "                        roi_cls_loss = torch.tensor(roi_cls_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['roi_classification_loss'] = roi_cls_loss\n",
        "\n",
        "                if 'roi_localization_loss' in roi_output:\n",
        "                    roi_loc_loss = roi_output['roi_localization_loss']\n",
        "                    if not isinstance(roi_loc_loss, torch.Tensor):\n",
        "                        roi_loc_loss = torch.tensor(roi_loc_loss, dtype=torch.float32, device=image.device)\n",
        "                    losses['roi_localization_loss'] = roi_loc_loss\n",
        "\n",
        "                # Compute total loss\n",
        "                total_loss = torch.tensor(0.0, dtype=torch.float32, device=image.device, requires_grad=True)\n",
        "                for loss_name, loss_value in losses.items():\n",
        "                    if loss_value.requires_grad:\n",
        "                        total_loss = total_loss + loss_value\n",
        "                    else:\n",
        "                        total_loss = total_loss + loss_value.detach().requires_grad_(True)\n",
        "\n",
        "                losses['total_loss'] = total_loss\n",
        "\n",
        "                print(f\"Debug: Training losses computed: {list(losses.keys())}\")\n",
        "                return losses\n",
        "\n",
        "            else:\n",
        "                # Inference mode - return detections\n",
        "                result = roi_output.copy()\n",
        "\n",
        "                # Transform boxes back to original size\n",
        "                if 'detections' in result:\n",
        "                    # Handle detection format - assuming it's a list of detections per image\n",
        "                    detections = result['detections']\n",
        "                    if isinstance(detections, list):\n",
        "                        for i, detection in enumerate(detections):\n",
        "                            if isinstance(detection, dict) and 'boxes' in detection:\n",
        "                                if detection['boxes'] is not None and len(detection['boxes']) > 0:\n",
        "                                    detection['boxes'] = transform_boxes_to_original_size(\n",
        "                                        detection['boxes'], image.shape[-2:], original_image_size\n",
        "                                    )\n",
        "                    elif isinstance(detections, dict) and 'boxes' in detections:\n",
        "                        if detections['boxes'] is not None and len(detections['boxes']) > 0:\n",
        "                            detections['boxes'] = transform_boxes_to_original_size(\n",
        "                                detections['boxes'], image.shape[-2:], original_image_size\n",
        "                            )\n",
        "\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in FasterRCNN forward pass: {e}\")\n",
        "            print(f\"Image shape: {image.shape}\")\n",
        "            if processed_target:\n",
        "                if isinstance(processed_target, list):\n",
        "                    print(f\"Target list length: {len(processed_target)}\")\n",
        "                    for i, tgt in enumerate(processed_target):\n",
        "                        print(f\"  Target {i} keys: {tgt.keys()}\")\n",
        "                        if 'bboxes' in tgt:\n",
        "                            print(f\"  Target {i} bboxes shape: {tgt['bboxes'].shape}\")\n",
        "                else:\n",
        "                    print(f\"Target keys: {processed_target.keys()}\")\n",
        "                    if 'bboxes' in processed_target and processed_target['bboxes'] is not None:\n",
        "                        print(f\"Bboxes shape: {processed_target['bboxes'].shape}\")\n",
        "                        print(f\"Bboxes content: {processed_target['bboxes']}\")\n",
        "\n",
        "            # Additional debugging for box dimension errors\n",
        "            import traceback\n",
        "            print(\"Full traceback:\")\n",
        "            traceback.print_exc()\n",
        "            raise"
      ],
      "metadata": {
        "id": "8m3Fbt3SJrn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-Nna3OGTcoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/config/tumor.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# Evaluate model\n",
        "model = FasterRCNN(config['model_params'], num_classes=config['dataset_params']['num_classes'])\n",
        "model.load_state_dict(torch.load('/content/lung_tumor_exp1/final_faster_rcnn_lung_tumor.pth'))\n",
        "evaluate_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBXI23Jz_Ig3",
        "outputId": "bb1b713e-12ed-4fbd-9dd4-7b6cd3e9335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:00<00:00, 9168.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 98 images found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/98 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1421, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1421, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   1%|          | 1/98 [00:00<00:17,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   2%|â–         | 2/98 [00:00<00:13,  7.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1096, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1096, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   3%|â–         | 3/98 [00:00<00:12,  7.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   4%|â–         | 4/98 [00:00<00:11,  7.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1482, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1482, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1455, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   5%|â–Œ         | 5/98 [00:00<00:11,  7.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1121, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1121, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1151, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   7%|â–‹         | 7/98 [00:00<00:10,  8.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1151, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   8%|â–Š         | 8/98 [00:00<00:10,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1428, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1428, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1237, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1237, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  10%|â–ˆ         | 10/98 [00:01<00:09,  9.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1157, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1157, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1181, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  11%|â–ˆ         | 11/98 [00:01<00:09,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1181, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  12%|â–ˆâ–        | 12/98 [00:01<00:08,  9.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1040, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1040, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1007, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  13%|â–ˆâ–        | 13/98 [00:01<00:08,  9.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1007, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1051, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1051, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1012, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  15%|â–ˆâ–Œ        | 15/98 [00:01<00:08,  9.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1012, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  16%|â–ˆâ–‹        | 16/98 [00:01<00:08,  9.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1387, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1387, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1097, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  17%|â–ˆâ–‹        | 17/98 [00:01<00:08,  9.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1097, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1140, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1140, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1420, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  19%|â–ˆâ–‰        | 19/98 [00:02<00:08,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1420, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  20%|â–ˆâ–ˆ        | 20/98 [00:02<00:08,  9.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1074, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1074, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1049, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  21%|â–ˆâ–ˆâ–       | 21/98 [00:02<00:07,  9.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1049, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1126, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  22%|â–ˆâ–ˆâ–       | 22/98 [00:02<00:07,  9.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1126, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1131, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  23%|â–ˆâ–ˆâ–       | 23/98 [00:02<00:07,  9.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1131, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1531, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  24%|â–ˆâ–ˆâ–       | 24/98 [00:02<00:08,  9.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1531, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1183, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  26%|â–ˆâ–ˆâ–Œ       | 25/98 [00:02<00:07,  9.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1183, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  27%|â–ˆâ–ˆâ–‹       | 26/98 [00:02<00:07,  9.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1413, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1413, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1129, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1129, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  29%|â–ˆâ–ˆâ–Š       | 28/98 [00:03<00:07,  9.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1103, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1103, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1028, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1028, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1167, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  31%|â–ˆâ–ˆâ–ˆ       | 30/98 [00:03<00:06,  9.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1167, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1173, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1173, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1388, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 33/98 [00:03<00:06,  9.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1388, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1165, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1165, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/98 [00:03<00:06,  9.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1123, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1123, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1082, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1082, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1101, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 37/98 [00:03<00:06, 10.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1101, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1090, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1090, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1356, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1356, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 39/98 [00:04<00:05, 10.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1307, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1307, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 41/98 [00:04<00:05,  9.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1319, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1319, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/98 [00:04<00:05,  9.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1442, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1442, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1326, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/98 [00:04<00:05,  9.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1326, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 47/98 [00:04<00:05,  9.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1378, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1378, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1465, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1465, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 49/98 [00:05<00:05,  9.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1409, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1409, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1341, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1341, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 51/98 [00:05<00:04,  9.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1286, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1286, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1412, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1412, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/98 [00:05<00:04,  9.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1537, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1537, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1300, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/98 [00:05<00:04,  9.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1300, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1364, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1364, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 57/98 [00:06<00:04,  9.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1471, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1471, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1434, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1434, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 59/98 [00:06<00:04,  9.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1329, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1329, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1332, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1332, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/98 [00:06<00:03,  9.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1305, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1305, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1399, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1399, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/98 [00:06<00:03,  9.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1445, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1445, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 65/98 [00:06<00:03,  9.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1299, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1299, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1416, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 67/98 [00:07<00:03,  9.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1416, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1404, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 69/98 [00:07<00:03,  9.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1281, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1281, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1304, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1304, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/98 [00:07<00:02,  9.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1371, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1371, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 74/98 [00:07<00:02,  9.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1302, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 76/98 [00:08<00:02,  9.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1321, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1321, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1414, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1414, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 77/98 [00:08<00:02,  9.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1366, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1366, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1377, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1377, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/98 [00:08<00:01,  9.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1384, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1384, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1334, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1334, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81/98 [00:08<00:01,  9.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1365, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1365, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1394, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1394, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/98 [00:08<00:01,  9.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1298, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1298, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1323, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1323, 4])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 85/98 [00:08<00:01,  9.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1213, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1213, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1325, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 87/98 [00:09<00:01,  9.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1325, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1361, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1361, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 89/98 [00:09<00:00,  9.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 2 boxes, shape: torch.Size([2, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1369, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 0 boxes, shape: torch.Size([0, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1472, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/98 [00:09<00:00,  9.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1472, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1296, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1296, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1261, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/98 [00:09<00:00,  9.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1261, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1398, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1398, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 95/98 [00:10<00:00,  9.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1425, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1312, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 97/98 [00:10<00:00,  9.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1289, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1350, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1350, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Debug: Processed target for 1 images\n",
            "  Image 0: 1 boxes, shape: torch.Size([1, 4])\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n",
            "Debug: RPN output keys: dict_keys(['proposals', 'scores', 'rpn_classification_loss', 'rpn_localization_loss'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections', 'roi_classification_loss', 'roi_localization_loss'])\n",
            "Debug: Training losses computed: ['rpn_classification_loss', 'rpn_localization_loss', 'roi_classification_loss', 'roi_localization_loss', 'total_loss']\n",
            "Debug: Features shape: torch.Size([1, 512, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:10<00:00,  9.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: RPN output keys: dict_keys(['proposals', 'scores'])\n",
            "Debug: Proposals - 1 batches, shapes: [torch.Size([1397, 4])]\n",
            "Debug: ROI output keys: dict_keys(['detections'])\n",
            "Evaluation Results:\n",
            "  Average Test Loss: 2.1101\n",
            "  Total Images: 98\n",
            "  Average Detections per Image: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_loss': 2.1101264029133078, 'total_images': 98, 'avg_detections': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}
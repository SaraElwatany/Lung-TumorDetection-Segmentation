{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQRKFA4xNH09YzwySnISs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraElwatany/Lung-TumorDetection-Segmentation/blob/main/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "L3nwzC9ycwgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdySwJH6cpiK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-lBBMXO7fRR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "BL9C_ZIjfBlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper Functions**"
      ],
      "metadata": {
        "id": "KsDfVPt9RtMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_tumor(image_path, model, detection=False):\n",
        "\n",
        "   \"\"\"\n",
        "    Perform tumor segmentation on an input image using a trained model.\n",
        "\n",
        "    This function supports two modes:\n",
        "    - Detection mode: expects a small cropped RGB tumor image and resizes it to 64x64.\n",
        "    - Full image segmentation mode: expects a grayscale image and resizes it to 256x256.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image file.\n",
        "        model (torch.nn.Module): Trained PyTorch model for tumor segmentation.\n",
        "        detection (bool, optional): If True, perform inference on a cropped tumor patch.\n",
        "                                    If False, perform segmentation on the full grayscale image. Default is False.\n",
        "\n",
        "    Returns:\n",
        "        None: Displays the input image and predicted mask using matplotlib.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  # Perform inference on the cropped tumor\n",
        "  if detection:\n",
        "\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert to PIL\n",
        "    image_pil = Image.fromarray(image)\n",
        "\n",
        "    # Resize\n",
        "    image_pil = image_pil.resize(64)\n",
        "\n",
        "    # Convert to normalized tensor\n",
        "    image = np.array(image_pil).astype(np.float32) / 255.0\n",
        "    image = np.transpose(image, (2, 0, 1))  # HWC to CHW\n",
        "    tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "\n",
        "\n",
        "  # Perform inference on the whole input image\n",
        "  else:\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\"L\")              # Convert to grayscale\n",
        "\n",
        "\n",
        "    # Define transform (same as training)\n",
        "    transform = T.Compose([\n",
        "                            T.Resize((256, 256)),  # adjust to match training input size\n",
        "                            T.ToTensor()\n",
        "                        ])\n",
        "\n",
        "\n",
        "    # Transform and add batch dimension\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)         # Shape: [1, 1, H, W]\n",
        "\n",
        "\n",
        "\n",
        "  # Forward pass\n",
        "  with torch.no_grad():\n",
        "      output = model(input_tensor)\n",
        "      output = torch.sigmoid(output)      # Convert logits to probabilities\n",
        "      pred_mask = (output > 0.5).float()  # Threshold for binary mask\n",
        "\n",
        "\n",
        "  # Visualize\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.title(\"Input Image\")\n",
        "  plt.imshow(image, cmap='gray')\n",
        "\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.title(\"Predicted Mask\")\n",
        "  plt.imshow(pred_mask.squeeze().cpu(), cmap='gray')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Hnoqzezzc4LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tumor Segmentation on Whole Image**"
      ],
      "metadata": {
        "id": "k_1AgXTVcy1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaGpeaiMOgy6"
      },
      "outputs": [],
      "source": [
        "def double_convolution(in_channels, out_channels):\n",
        "    \"\"\"\n",
        "    In the original paper implementation, the convolution operations were\n",
        "    not padded but we are padding them here. This is because, we need the\n",
        "    output result size to be same as input size.\n",
        "    \"\"\"\n",
        "    conv_op = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                            nn.ReLU(inplace=True),\n",
        "\n",
        "                            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                            nn.ReLU(inplace=True)\n",
        "                           )\n",
        "    return conv_op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2zajPGeOEMu"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Contracting path.\n",
        "\n",
        "        # Each convolution is applied twice.\n",
        "        self.down_convolution_1 = double_convolution(1, 64)\n",
        "        self.down_convolution_2 = double_convolution(64, 128)\n",
        "        self.down_convolution_3 = double_convolution(128, 256)\n",
        "        self.down_convolution_4 = double_convolution(256, 512)\n",
        "        self.down_convolution_5 = double_convolution(512, 1024)\n",
        "\n",
        "\n",
        "        # Expanding path.\n",
        "        self.up_transpose_1 = nn.ConvTranspose2d(\n",
        "                                                in_channels=1024, out_channels=512,\n",
        "                                                kernel_size=2,\n",
        "                                                stride=2)\n",
        "\n",
        "        # Below, `in_channels` again becomes 1024 as we are concatinating.\n",
        "        self.up_convolution_1 = double_convolution(1024, 512)\n",
        "\n",
        "        self.up_transpose_2 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=512, out_channels=256,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_2 = double_convolution(512, 256)\n",
        "\n",
        "        self.up_transpose_3 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=256, out_channels=128,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_3 = double_convolution(256, 128)\n",
        "\n",
        "        self.up_transpose_4 = nn.ConvTranspose2d(\n",
        "                                                  in_channels=128, out_channels=64,\n",
        "                                                  kernel_size=2,\n",
        "                                                  stride=2)\n",
        "\n",
        "        self.up_convolution_4 = double_convolution(128, 64)\n",
        "\n",
        "        # output => `out_channels` as per the number of classes.\n",
        "        self.out = nn.Conv2d(\n",
        "                              in_channels=64, out_channels=num_classes,\n",
        "                              kernel_size=1\n",
        "                            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        down_1 = self.down_convolution_1(x)\n",
        "        down_2 = self.max_pool2d(down_1)\n",
        "        down_3 = self.down_convolution_2(down_2)\n",
        "        down_4 = self.max_pool2d(down_3)\n",
        "        down_5 = self.down_convolution_3(down_4)\n",
        "        down_6 = self.max_pool2d(down_5)\n",
        "        down_7 = self.down_convolution_4(down_6)\n",
        "        down_8 = self.max_pool2d(down_7)\n",
        "        down_9 = self.down_convolution_5(down_8)\n",
        "\n",
        "        # *** DO NOT APPLY MAX POOL TO down_9 ***\n",
        "\n",
        "        up_1 = self.up_transpose_1(down_9)\n",
        "        x = self.up_convolution_1(torch.cat([down_7, up_1], 1))\n",
        "\n",
        "        up_2 = self.up_transpose_2(x)\n",
        "        x = self.up_convolution_2(torch.cat([down_5, up_2], 1))\n",
        "\n",
        "        up_3 = self.up_transpose_3(x)\n",
        "        x = self.up_convolution_3(torch.cat([down_3, up_3], 1))\n",
        "\n",
        "        up_4 = self.up_transpose_4(x)\n",
        "        x = self.up_convolution_4(torch.cat([down_1, up_4], 1))\n",
        "\n",
        "        out = self.out(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgDb6CdCqjcF"
      },
      "outputs": [],
      "source": [
        "whole_img_model = UNet(num_classes=1).to(device)\n",
        "whole_img_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/unet_lung_segmentation_0.002502174032005397.pth\", map_location=device))\n",
        "whole_img_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment_tumor_whole_img(image_path, true_mask_path, whole_img_model)"
      ],
      "metadata": {
        "id": "xOpIBOSKe59t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tumor Detection**"
      ],
      "metadata": {
        "id": "vcyM1akHdWt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_tumor():\n",
        "\n",
        "  # Save a PDF file that has information about the existence/nonexistence of tumor, its location, size and number of fragments.\n",
        "  pass"
      ],
      "metadata": {
        "id": "TwQaHloqdaGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Segmentation of detected tumors**"
      ],
      "metadata": {
        "id": "dEmha2z7daf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.15),  # Slightly increased dropout\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "BWmONLnBR0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[32, 64, 128, 256]):\n",
        "        super().__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Downsampling path (encoder)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "\n",
        "        # Upsampling path (decoder)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)  # transpose conv\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            x = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](x)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "R2eM4nkUR1OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_detection = UNet().to(device)\n",
        "model_detection.load_state_dict(torch.load(f\"/content/drive/MyDrive/unet_lung_segmentation_0.002502174032005397.pth\", map_location=device))\n",
        "model_detection.to(device)"
      ],
      "metadata": {
        "id": "E6gpxy90SRXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_tumor(image_path, true_mask_path, model_detection, detection=True)"
      ],
      "metadata": {
        "id": "DcmYC_vZdf69"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}